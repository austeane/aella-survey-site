This file is a merged representation of a subset of the codebase, containing files not matching ignore patterns, combined into a single document by Repomix.
The content has been processed where security check has been disabled.

<file_summary>
This section contains a summary of this file.

<purpose>
This file contains a packed representation of a subset of the repository's contents that is considered the most important context.
It is designed to be easily consumable by AI systems for analysis, code review,
or other automated processes.
</purpose>

<file_format>
The content is organized as follows:
1. This summary section
2. Repository information
3. Directory structure
4. Repository files (if enabled)
5. Multiple file entries, each consisting of:
  - File path as an attribute
  - Full contents of the file
</file_format>

<usage_guidelines>
- This file should be treated as read-only. Any changes should be made to the
  original repository files, not this packed version.
- When processing this file, use the file path to distinguish
  between different files in the repository.
- Be aware that this file may contain sensitive information. Handle it with
  the same level of security as you would the original repository.
</usage_guidelines>

<notes>
- Some files may have been excluded based on .gitignore rules and Repomix's configuration
- Binary files are not included in this packed representation. Please refer to the Repository Structure section for a complete list of file paths, including binary files
- Files matching these patterns are excluded: data/**, Big Kink Survey*, src/lib/schema/columns.generated.json, public/**, **/*.test.ts, pnpm-lock.yaml, .husky/**, design-mockups/**, BKSPublic_column_notes.txt, BKSPublic.parquet, src/routeTree.gen.ts, docs/schema/survey-summary.md
- Files matching patterns in .gitignore are excluded
- Files matching default ignore patterns are excluded
- Security check has been disabled - content may contain sensitive information
- Files are sorted by Git change count (files with more changes are at the bottom)
</notes>

</file_summary>

<directory_structure>
docs/
  design/
    architecture.md
    deployment.md
    frontend.md
    mcps.md
  plans/
    completed/
      2026-02-12-plan-continuation.md
  schema/
    data-exploration.md
    README.md
  worklog.md
mcp-server/
  Dockerfile
  pyproject.toml
  server.py
scripts/
  profile-schema.mjs
  sync-public-data.mjs
src/
  lib/
    api/
      contracts.ts
    client/
      api.ts
    duckdb/
      init.ts
      provider.tsx
      sql-helpers.ts
      use-query.ts
    schema/
      caveats.ts
      metadata.ts
      types.ts
    server/
      api-response.ts
      db.ts
      sql-guards.ts
  routes/
    api/
      crosstab.ts
      health.ts
      query.ts
      schema.ts
      stats.$column.ts
    __root.tsx
    explore.tsx
    index.tsx
    profile.tsx
    sql.tsx
  router.tsx
  styles.css
.gitignore
.mcp.json
.npmrc
CLAUDE.md
META-PLAN.md
package.json
PLAN.md
repomix-notes.md
tsconfig.json
vite.config.ts
</directory_structure>

<files>
This section contains the contents of the repository's files.

<file path="docs/design/frontend.md">
# Frontend Design System — "Ink & Paper"

Direction: editorial research journal. The UI should feel like a beautifully typeset academic publication — warm, authoritative, and clear. Think NYT data journalism meets a well-set research paper.

## Design Tokens

### Colors (CSS variables)

```css
--paper: #f5f0e8;          /* primary background */
--paper-warm: #ede6d8;      /* secondary/sidebar background */
--ink: #1a1612;             /* primary text */
--ink-light: #4a4238;       /* secondary text */
--ink-faded: #8a7e70;       /* tertiary/muted text */
--rule: #c8bfb0;            /* borders, horizontal rules */
--rule-light: #ddd5c8;      /* subtle dividers */
--accent: #b8432f;          /* emphasis, links, active states */
--accent-hover: #9a3625;    /* accent hover/pressed */
--highlight: #e8d5a0;       /* selection, highlight backgrounds */
--sidebar-bg: #eae3d5;      /* raised panel backgrounds */
```

**Principle**: Dominant warm cream with ink-dark text. Red accent used sparingly — section numbers, active nav, percentage highlights. No gradients. No neon. No purple.

### Typography

| Role | Font | Weight | Size | Tracking |
|---|---|---|---|---|
| Display / H1 | Fraunces | 700 | 2.75rem | -0.03em |
| Section headers | Fraunces | 600 | 1.2rem | normal |
| Body text | Source Serif 4 | 400 | 1rem (16px) | normal |
| Body emphasis | Source Serif 4 | 600 | 0.95rem | normal |
| Subtitle/italic | Source Serif 4 italic | 400 | 1.1rem | normal |
| Data / mono | JetBrains Mono | 400-500 | 0.8rem | normal |
| Labels / caps | JetBrains Mono | 400 | 0.65rem | 0.12em, uppercase |
| Large numbers | Fraunces | 700 | 2.5rem | -0.03em |

**Google Fonts import**:
```
Fraunces:ital,opsz,wght@0,9..144,300;0,9..144,400;0,9..144,600;0,9..144,700;1,9..144,400
Source+Serif+4:ital,opsz,wght@0,8..60,300;0,8..60,400;0,8..60,600;1,8..60,400
JetBrains+Mono:wght@400;500
```

**Rules**:
- Never use system fonts, Inter, Roboto, or Arial
- Fraunces for headlines and large numbers only — never body
- Source Serif 4 for all body/paragraph text
- JetBrains Mono for data values, column names, SQL, code, and all-caps labels
- Pair font sizes with appropriate line-height: display 1.1, body 1.6, data 1.4

### Spacing

Base unit: `1rem` (16px). Use multiples: 0.5, 0.75, 1, 1.5, 2, 3.

- Page padding: `3rem 2rem` (top/bottom, left/right)
- Section gaps: `3rem` between major sections
- Card padding: `1.5rem 2rem`
- Table cell padding: `0.6rem 0`
- Max content width: `1200px`, centered

### Borders & Rules

- Primary divider: `2px solid var(--ink)` (nav bottom, section separators)
- Section divider: `1px solid var(--rule)` (between content areas)
- Table header border: `2px solid var(--ink)` bottom
- Table row borders: `1px solid var(--rule-light)` bottom
- Card borders: `1px solid var(--rule)` or `1px solid var(--ink)` for emphasis
- **No border-radius anywhere** — square corners are part of the editorial identity

### Backgrounds & Texture

- Apply a subtle paper noise texture via SVG filter overlay on `body::before` at ~3% opacity
- No solid color cards — use `var(--paper)` base with `var(--sidebar-bg)` for raised panels
- The accent bar on stat sections: a `3px` solid red line above the panel
- No box shadows. Depth comes from borders and background contrast only.

## Component Patterns

### Navigation

- Horizontal bar with `2px solid var(--ink)` bottom border
- Logo: Fraunces 700, 1.5rem. Subtitle in Source Serif italic, faded
- Links: Source Serif 0.85rem, uppercase with 0.06em tracking
- Active state: `var(--accent)` color + `2px` bottom border in accent
- No pills, no background highlights — editorial underline style

### Stat Cards

- Grid row with no gaps, shared `1px solid var(--ink)` outer border
- Internal dividers: `1px solid var(--rule)` between cells
- Label: JetBrains Mono, 0.65rem, uppercase, faded
- Value: Fraunces 700, 2.5rem
- Optional note: Source Serif italic, 0.8rem, faded

### Data Tables

- Full-width, no outer border
- Header: JetBrains Mono, 0.65rem, uppercase, faded. Bottom border `2px solid var(--ink)`
- Rows: `1px solid var(--rule-light)` bottom. No zebra striping
- First column (names): JetBrains Mono, ink color
- Numeric columns: JetBrains Mono, right-aligned
- Optional inline bar: 4px height, accent color at 30% opacity, positioned absolutely

### Section Headers

- Fraunces 600, 1.2rem
- Bottom border: `1px solid var(--rule)`
- Numbered: JetBrains Mono section number in accent color (e.g., "01", "02")
- Use `display: flex; align-items: baseline; gap: 0.75rem;`

### Form Controls (selects, inputs)

- JetBrains Mono, 0.8rem
- `1px solid var(--rule)` border, `var(--paper)` background
- No border-radius
- Custom dropdown arrow via `::after` pseudo-element
- Focus: border-color to `var(--ink)`

### Raised Panels (Column Inspector, etc.)

- Background: `var(--sidebar-bg)`
- Border: `1px solid var(--rule)`
- Top accent bar: `3px solid var(--accent)` via `::before` pseudo-element
- Internal stat grids: `1px` gap with `var(--rule)` background showing through

### Caveats / Callouts

- No background color — just content with rule dividers
- Title: Source Serif 600, 0.95rem
- Body: Source Serif 400, 0.85rem, `var(--ink-light)`
- Guidance: Source Serif italic, 0.75rem, `var(--ink-faded)`

## Animation

- Entrance: `fadeUp` — opacity 0→1, translateY 12px→0, 0.6s ease-out
- Stagger delays: 0.1s increments for sequential sections
- No hover animations on cards (editorial, not SaaS)
- Transitions on nav links: color 0.2s

## Anti-Patterns — Never Do These

- **No dark mode** — this is a light, warm design. The paper IS the identity
- **No border-radius** — square corners everywhere
- **No gradients** — flat colors, borders, and typography create hierarchy
- **No box-shadows** — depth via background contrast and rules
- **No Inter/Roboto/system fonts** — always Fraunces + Source Serif + JetBrains
- **No purple, blue, or neon accents** — only red (`#b8432f`) as accent
- **No pill buttons or rounded chips** — square, bordered elements
- **No emoji or icons in headers** — numbered sections and typography only
- **No card hover lift effects** — this is a journal, not a dashboard

## Reference Mockup

The canonical mockup is at `design-mockups/01-ink-and-paper.html`. Open it in a browser to see the full dashboard rendered in this design system. All implementation should match this aesthetic.
</file>

<file path="docs/schema/data-exploration.md">
# BKS Public Dataset: Deep Exploration Summary

Generated: 2026-02-12

## 1. Overview

| Property | Value |
|----------|-------|
| File | `data/BKSPublic.parquet` |
| Rows | 15,503 |
| Columns | 365 |
| Format | Apache Parquet |
| Sample | ~1.6% stratified subsample of ~970k total responses |

The Big Kink Survey (BKS) is a large-scale, self-selected online survey studying human sexual preferences, fetishes, personality traits, childhood experiences, and demographics. The public dataset represents a stratified subsample of the full ~970k-response dataset, balanced across age and sex.

## 2. Survey Methodology & Structure

### Design
- **Platform**: Online self-administered survey ("GT" / Google-based tool)
- **5 parts**: Demographics (1), Childhood (2), Sex fantasy intro + preferences (3), Kink/fetish gate + details (4-5)
- **Gated questions**: Many questions are conditional. If respondents don't pass an interest gate (e.g., checking "bondage" as an interest), they skip all sub-questions for that category. These respondents show as NULL/NA and can often be treated as implicit 0s.
- **Scales**: Originally 0-8, cleaned to 0-5 or 1-5 in the processed dataset.
- **Questions added over time**: The survey evolved as responses accumulated. Some questions were added late (noted in the survey doc), creating structural missingness for early respondents.

### Sampling
- The public file is a ~1.6% **stratified subsample** designed to produce even distributions across age bins and biological sex.
- Source platform was primarily Reddit, FetLife, Twitter, Facebook, Discord, Tumblr, and others (the referral source column was dropped in this public release).

### Data Cleaning Applied
- Several columns were **binned/collapsed** (politics from 5 to 3 levels, BMI from 10 to 2, straightness from continuous to 5 levels, then further to 2, etc.)
- Some columns were **combined** (childhood_adversity merges abuse + childhood sexual assault; childhood_gender_tolerance averages two separate tolerance questions)
- **Computed columns** were added (OCEAN personality scores, powerlessness, total fetish counts)
- **Dropped columns**: country, ethnicity, religion_importance, sex age, social class, detailed gender identity, relationship status, sex work questions, pedophilia sub-questions, abuse detail sub-questions, all "*smost" sparse columns, duplicate GT columns

## 3. Column Types & Structure

### DuckDB Storage Types
| Type | Count |
|------|-------|
| VARCHAR | 213 |
| DOUBLE | 152 |
| **Total** | **365** |

### Logical Types (from schema profiler)
| Logical Type | Count | Description |
|-------------|-------|-------------|
| categorical | 251 | Ordinal scales, enums, binned values (most survey responses) |
| text | 62 | Free-form categorical with many unique values (fetish sub-item picks, "most erotic" selections) |
| numeric | 52 | Continuous or high-cardinality numbers (OCEAN scores, totals, slider values) |

### Tag Distribution (from schema profiler)
| Tag | Count | Description |
|-----|-------|-------------|
| fetish | 196 | Kink/fetish interest ratings, sub-items, onset ages |
| other | 106 | Columns not clearly in another category |
| demographic | 64 | Age, sex, orientation, politics, childhood, personality |
| derived | 48 | Computed/aggregated columns (totals, averages) |
| ocean | 5 | Big Five personality trait scores |

### Column Groups by Pattern

| Category | Count | Examples |
|----------|-------|---------|
| Fetish sub-item details | ~90 | Specific items within each fetish category (bondage types, nonconsent scenarios, etc.) |
| "Most erotic" picks | ~50 | Within each category, which specific item is most arousing |
| Total/count columns | 36 | Aggregated counts per category (Totalbondage, Totalnonconsent1, etc.) |
| Fetish onset ages | 30 | "How old were you when you first experienced interest in X?" |
| Demographic/background | ~24 | age, biomale, politics, straightness, childhood questions |
| Fetish arousal ratings | ~20 | "I find X to be:" on the arousal scale |
| OCEAN personality | 6 | opennessvariable, conscientiousnessvariable, etc. |

## 4. Demographic Distributions

### Age (binned, stratified)
| Age Bin | N | % |
|---------|---|---|
| 14-17 | 3,197 | 20.6% |
| 18-20 | 2,532 | 16.3% |
| 21-24 | 3,211 | 20.7% |
| 25-28 | 3,278 | 21.1% |
| 29-32 | 3,285 | 21.2% |

Age was binned into 5 groups. The distribution is nearly uniform due to stratified sampling. Note: the survey includes minors (14-17), who comprise ~21% of the sample.

### Biological Sex
| Value | N | % |
|-------|---|---|
| biomale = 0 (female) | 7,564 | 48.8% |
| biomale = 1 (male) | 7,939 | 51.2% |

Nearly 50/50 split — this is a result of the stratified sampling. The original "gender" column in the survey had 5 options (Man cis, Man trans, Woman cis, Woman trans, Nonbinary AMAB/AFAB) but the public dataset only retains `biomale` (assigned sex at birth).

### Sexual Orientation (straightness)
| Value | N | % |
|-------|---|---|
| Straight | 13,705 | 88.4% |
| Not straight | 1,798 | 11.6% |

Originally a 5-level scale (Gay, Leaning gay, Bi, Leaning straight, Straight), collapsed to binary in this dataset.

### Politics
| Value | N | % |
|-------|---|---|
| Moderate | 5,599 | 36.1% |
| Liberal | 5,046 | 32.5% |
| Conservative | 4,858 | 31.3% |

Collapsed from a 5-level scale. Remarkably even distribution, likely reflecting the stratified sample.

### BMI
| Value | N | % |
|-------|---|---|
| Overweight+ | 8,923 | 57.6% |
| Not overweight | 6,580 | 42.4% |

Collapsed from 10 original bins to binary.

### Relationship Style
| Value | N | % |
|-------|---|---|
| Monogamous | 11,720 | 75.6% |
| Not monogamous | 3,780 | 24.4% |

Collapsed from 3 original levels (Mono, Middle, Poly).

### Sexual Partner Count
| Value | N | % |
|-------|---|---|
| 0 | 4,524 | 29.2% |
| 1-2 | 3,904 | 25.2% |
| 3-7 | 3,178 | 20.5% |
| 8-20 | 2,294 | 14.8% |
| 21+ | 1,363 | 8.8% |
| NULL | 240 | 1.5% |

Nearly 30% of respondents report zero sexual partners, consistent with the young age distribution (21% are 14-17).

### Mental Illness
| Value | N | % |
|-------|---|---|
| None | 9,307 | 60.0% |
| Any | 6,196 | 40.0% |

40% report at least one moderate-to-severe mental health condition. The original survey offered 19 checkbox options (ADHD, Anxiety, Autism, Depression, PTSD, etc.); the public dataset collapses these to binary.

### Childhood Adversity
| Value | N | % |
|-------|---|---|
| None | 10,617 | 68.5% |
| Any | 4,886 | 31.5% |

Combined from abuse and childhood sexual assault questions into binary.

### Adult Sexual Assault
| Value | N | % |
|-------|---|---|
| No | 10,825 | 69.8% |
| Yes | 4,675 | 30.2% |

Originally 4-level (No, Yes mild/moderate/severe), collapsed to binary in this dataset.

### Childhood Gender Tolerance
| Value | N | % |
|-------|---|---|
| Medium | 8,603 | 55.5% |
| Intolerant | 4,150 | 26.8% |
| Tolerant | 2,739 | 17.7% |

Computed from two questions about childhood culture's tolerance for gender role violations and genderbending.

### Upbringing Sexual Liberation
| Value | N | % |
|-------|---|---|
| Repressed | 6,767 | 43.6% |
| Liberated | 5,427 | 35.0% |
| Neutral | 3,302 | 21.3% |

Collapsed from 7 levels to 3.

## 5. Sexual Preference Distributions

### Dominance/Submission Self-ID
| Value | N | % |
|-------|---|---|
| Switch/equal/no preference | 4,354 | 28.1% |
| Moderately submissive | 2,767 | 17.8% |
| Moderately dominant | 2,261 | 14.6% |
| Slightly submissive | 1,763 | 11.4% |
| Slightly dominant | 1,476 | 9.5% |
| Totally submissive | 1,469 | 9.5% |
| Totally dominant | 1,005 | 6.5% |

Submissive responses (38.7%) outnumber dominant (30.6%), with switches at 28.1%.

### Consent Preference in Fantasy
| Value | N | % |
|-------|---|---|
| Full, enthusiastic consent | 6,709 | 43.3% |
| Mostly consenting, slightly nonconsenting | 4,211 | 27.2% |
| Equally consenting and nonconsenting | 2,553 | 16.5% |
| Mostly nonconsenting, slightly consenting | 1,303 | 8.4% |
| Full nonconsent | 721 | 4.7% |

Over half (56.7%) prefer at least some nonconsent element in fantasy.

### Sexual Interest Broadness
| Value | N | % |
|-------|---|---|
| Somewhat broad | 3,687 | 23.8% |
| Equally narrow and broad | 3,159 | 20.4% |
| A little broad | 2,603 | 16.8% |
| Very broad | 2,392 | 15.4% |
| A little narrow | 1,754 | 11.3% |
| Somewhat narrow | 1,261 | 8.1% |
| Very narrow | 643 | 4.1% |

Skewed toward broad — 56% describe their interests as at least a little broad.

### Porn/Erotica Consumption
| pornhabit | Label | N | % |
|-----------|-------|---|---|
| 9 | Multiple times/day | 1,619 | 10.4% |
| 8 | Daily | 2,659 | 17.2% |
| 7 | Multiple times/week | 4,513 | 29.1% |
| 6 | Once a week | 1,409 | 9.1% |
| 5 | Few times/month | 1,881 | 12.1% |
| 4 | Once a month | 678 | 4.4% |
| 3 | Few times/year | 752 | 4.9% |
| 2 | Once a year | 312 | 2.0% |
| 1 | Less than once/year | 341 | 2.2% |
| 0 | Never | 1,339 | 8.6% |

56.7% consume porn/erotica at least multiple times per week. Only 8.6% report never consuming it.

### Fetish Category Counts
| Metric | Common (17 cats) | Uncommon (13 cats) | Total (30 cats) | Sex Acts |
|--------|------|---------|-------|----------|
| Mean | 7.5 | 2.7 | 10.0 | 7.2 |
| Median | — | — | 9.0 | — |
| Min | — | — | 0.0 | — |
| Max | — | — | 30.0 | — |

On average, respondents check interest in 10 of 30 fetish categories (7.5 "common" + 2.7 "uncommon").

### Most Erotic Self-Feeling (youfeelmost)
| Emotion | N | % |
|---------|---|---|
| Eagerness or desire | 4,974 | 33.2% |
| Love or romance | 3,004 | 20.1% |
| Wildness or primalness | 1,858 | 12.4% |
| Powerlessness or vulnerability | 1,739 | 11.6% |
| Safety or warmth | 978 | 6.5% |
| Power or smugness | 729 | 4.9% |

### Most Erotic Other-Feeling (otherfeel1most)
| Emotion | N | % |
|---------|---|---|
| Eagerness or desire | 4,593 | 31.4% |
| Love or romance | 2,549 | 17.4% |
| Wildness or primalness | 1,767 | 12.1% |
| Power or smugness | 1,742 | 11.9% |
| Powerlessness or vulnerability | 1,099 | 7.5% |
| Safety or warmth | 918 | 6.3% |

Notable: when imagining the other person's feelings, "power or smugness" rises significantly (4.9% self vs 11.9% other).

### Masturbation Onset Age
| Age | N | % |
|-----|---|---|
| 12-13 | 4,822 | 31.1% |
| 10-11 | 2,909 | 18.8% |
| 14-15 | 2,426 | 15.6% |
| 8-9 | 1,353 | 8.7% |
| 7 or younger | 1,204 | 7.8% |
| 16-17 | 1,081 | 7.0% |
| 18+ | 1,025 | 6.6% |
| Never | 678 | 4.4% |

Peak onset at 12-13, with 67% beginning by age 13.

## 6. OCEAN Personality & Powerlessness

Mean scores across the sample (range -6 to +6 for OCEAN, -9 to +9 for powerlessness):

| Trait | Mean |
|-------|------|
| Openness | +1.64 |
| Conscientiousness | +1.27 |
| Extroversion | -1.29 |
| Neuroticism | +0.96 |
| Agreeableness | +2.00 |
| Powerlessness | +0.70 |

The sample skews toward **introverted** (extroversion is the only negative mean), moderately open and agreeable, slightly neurotic, and slightly powerless.

## 7. Data Quality & Missingness

### Null Rate Distribution
| Null Rate Bucket | Column Count |
|-----------------|--------------|
| 0% (asked of everyone, always answered) | 19 |
| >0% to 10% | 51 |
| 10% to 25% | 13 |
| 25% to 50% | 48 |
| 50% to 75% | 59 |
| 75% to 90% | 91 |
| 90% to 100% | 84 |

**Key observation**: Only 19 columns have zero nulls. The vast majority of columns (234 of 365, or 64%) have >50% null rates. This is **by design** — the survey uses gated questions extensively. If you don't check "bondage" as an interest, you never see the 10+ bondage sub-questions, so those columns are NULL for you.

### Structural Missingness Patterns

1. **Gated fetish sub-questions** (largest source of NULLs): Each fetish category has a gate checkbox. Respondents who don't check the gate have NULLs for all sub-questions in that category. These NULLs mean "not interested" and can be treated as implicit 0s for analysis.

2. **Questions added late**: Some questions were added mid-survey (noted with dates in the survey doc). Earlier respondents never saw these questions, creating time-based structural missingness. Examples:
   - Menstrual cycle, hormonal BC, PMS questions (added Nov 2024)
   - Sex work questions (added Oct 2024)
   - Horny-right-now questions (added May 2025)
   - Blanchard-style body questions (added ~Mar 2025)
   - Narcissism question (added late, 91.6% null)

3. **Sex-gated questions**: Some questions only appear for biomale=0 (vaginal orgasm, hookups, breakups) or biomale=1 (circumcision). These show ~50% NULLs reflecting the ~50/50 sex split.

4. **Deeply nested gates**: Some questions are gated behind 2-3 levels of gates. For example, pedophilia sub-questions (dropped in this dataset) required: checking "Age" interest -> rating arousal > threshold -> choosing "Older" in age gap -> then sub-questions appeared.

### Zero-Null Columns (19 universal columns)
These were asked of everyone and always answered:
- `age`, `biomale`, `straightness`, `politics`, `bmi`
- OCEAN personality (5 computed scores): openness, conscientiousness, extroversion, neuroticism, agreeableness
- `powerlessnessvariable`
- `childhood_adversity`, `TotalMentalIllness`
- `pornhabit`, `totalfetishcategory`
- `knowwhatarousesyou`, `normalsex`, `cunnilingus`
- Shame question: "I am ashamed or embarrassed about at least some of what arouses me"

### Highest-Null Columns (>95%)
These represent deeply gated, rare interests or late-added questions:
- Smegma, vomit, scat sub-items (~99% null)
- Body horror, necrophilia sub-items (~98-99% null)
- Messiness sub-items (~99% null)
- Baby fever question (~99% null)
- Grandparent/grandchild incest preferences (~98% null)

## 8. Notable Survey Design Observations

1. **Stratified subsample**: The even age/sex distribution is artificial. The full dataset (~970k) likely has very different demographic distributions (probably skewing younger, male, and from Reddit/internet communities).

2. **Self-selection bias**: Respondents sought out this survey (from Reddit, FetLife, etc.), so this population is not representative of the general public. They are people interested enough in kink/sexuality to complete a lengthy survey.

3. **Fantasy framing**: The survey explicitly tells respondents to answer about fantasies, not real-life behavior. The consent preferences, for example, ask about "preferred erotic scenarios" not actual practice.

4. **Scale compression**: Original 0-8 arousal scales were compressed to 0-5, and many demographic variables were heavily binned (7 -> 3 levels, 10 -> 2 levels). This loses granularity but aids privacy and simplifies analysis.

5. **Computed columns**: OCEAN scores are computed as differences of agreement-scale items (e.g., openness = "I have excellent ideas" - "I have difficulty understanding abstract ideas"). Powerlessness sums three related items. These are rough proxies, not validated psychometric instruments.

6. **Dropped sensitive data**: Country, ethnicity, religion_importance, detailed gender identity, relationship status, and sex work data were all dropped for the public release. This limits certain analyses but protects respondent privacy.

7. **Column naming**: Columns use a mix of:
   - Short lowercase names (`age`, `biomale`, `straightness`)
   - Full question text with GT IDs (`"I find blowjobs:" (yuc275j)`)
   - Computed names (`opennessvariable`, `totalfetishcategory`)
   - This inconsistency means column names need careful handling in queries.

8. **Arousal scales are nonlinear**: The arousal scale maps to: Not arousing=0, Slightly=1, Somewhat=2, Moderately=3, Very=5, Extremely=8. The jumps from 3->5 and 5->8 are intentional. "Vanilla" items use a reversed scale (0 to -8).

## 9. Summary for AI Agents

When querying this dataset:
- **NULL handling is critical**: Most NULLs mean "question not shown" (gated out), not "refused to answer." For fetish items, NULL ~= 0 (not interested).
- **Column names with quotes/special chars**: Many column names contain double quotes, parentheses, and colons. Use DuckDB's double-quote escaping: `"column ""with quotes"" (id)"`.
- **Demographics are balanced by design**: Don't interpret even age/sex distributions as reflecting the true survey population.
- **19 universal columns** (0% null) are the safest starting points for analysis.
- **Fetish data follows a gate -> rate -> detail pattern**: Gate checkbox (in Totalfetish1/2) -> arousal rating (0-8 scale) -> sub-item details (specific scenarios/preferences) -> onset age -> "most erotic" pick.
- **The `totalfetishcategory` column** (0-30 range, mean=10, median=9) is a useful aggregate measure of kink breadth.
</file>

<file path="src/lib/duckdb/init.ts">
import * as duckdb from "@duckdb/duckdb-wasm";

import duckdbMvpWasm from "@duckdb/duckdb-wasm/dist/duckdb-mvp.wasm?url";
import duckdbMvpWorker from "@duckdb/duckdb-wasm/dist/duckdb-browser-mvp.worker.js?url";
import duckdbEhWasm from "@duckdb/duckdb-wasm/dist/duckdb-eh.wasm?url";
import duckdbEhWorker from "@duckdb/duckdb-wasm/dist/duckdb-browser-eh.worker.js?url";

const PARQUET_URL = "/BKSPublic.parquet";

let dbPromise: Promise<duckdb.AsyncDuckDB> | null = null;

async function createDb(): Promise<duckdb.AsyncDuckDB> {
  const bundles: duckdb.DuckDBBundles = {
    mvp: {
      mainModule: duckdbMvpWasm,
      mainWorker: duckdbMvpWorker,
    },
    eh: {
      mainModule: duckdbEhWasm,
      mainWorker: duckdbEhWorker,
    },
  };

  const bundle = await duckdb.selectBundle(bundles);

  const logger = new duckdb.ConsoleLogger();
  const worker = new Worker(bundle.mainWorker!);
  const db = new duckdb.AsyncDuckDB(logger, worker);
  await db.instantiate(bundle.mainModule, bundle.pthreadWorker);

  await db.registerFileURL("BKSPublic.parquet", PARQUET_URL, duckdb.DuckDBDataProtocol.HTTP, false);

  const conn = await db.connect();
  await conn.query(
    `CREATE OR REPLACE VIEW data AS SELECT * FROM read_parquet('BKSPublic.parquet')`,
  );
  await conn.close();

  return db;
}

export function getDuckDB(): Promise<duckdb.AsyncDuckDB> {
  if (!dbPromise) {
    dbPromise = createDb();
  }
  return dbPromise;
}
</file>

<file path="src/lib/duckdb/provider.tsx">
import { createContext, useContext, useEffect, useState } from "react";
import type { AsyncDuckDB } from "@duckdb/duckdb-wasm";

import { getDuckDB } from "./init";

interface DuckDBContextValue {
  db: AsyncDuckDB | null;
  loading: boolean;
  error: string | null;
}

const DuckDBContext = createContext<DuckDBContextValue>({
  db: null,
  loading: true,
  error: null,
});

export function DuckDBProvider({ children }: { children: React.ReactNode }) {
  const [db, setDb] = useState<AsyncDuckDB | null>(null);
  const [loading, setLoading] = useState(true);
  const [error, setError] = useState<string | null>(null);

  useEffect(() => {
    let cancelled = false;

    getDuckDB()
      .then((instance) => {
        if (!cancelled) {
          setDb(instance);
          setLoading(false);
        }
      })
      .catch((err: unknown) => {
        if (!cancelled) {
          setError(err instanceof Error ? err.message : "Failed to initialize DuckDB-WASM");
          setLoading(false);
        }
      });

    return () => {
      cancelled = true;
    };
  }, []);

  return (
    <DuckDBContext value={{ db, loading, error }}>
      {children}
    </DuckDBContext>
  );
}

export function useDuckDB(): DuckDBContextValue {
  return useContext(DuckDBContext);
}
</file>

<file path="src/lib/duckdb/sql-helpers.ts">
export function quoteIdentifier(identifier: string): string {
  return `"${identifier.replaceAll('"', '""')}"`;
}

export function quoteLiteral(value: string | number | boolean | null): string {
  if (value === null) return "NULL";
  if (typeof value === "number") return String(value);
  if (typeof value === "boolean") return value ? "TRUE" : "FALSE";
  return `'${value.replaceAll("'", "''")}'`;
}

export type FilterValue = string | number | boolean | null;
export type FilterInput = FilterValue | FilterValue[];

export function buildWhereClause(filters: Record<string, FilterInput> | undefined): string {
  if (!filters || Object.keys(filters).length === 0) return "";

  const predicates: string[] = [];

  for (const [columnName, rawValue] of Object.entries(filters)) {
    const col = quoteIdentifier(columnName);

    if (Array.isArray(rawValue)) {
      const nonNull = rawValue.filter((v) => v !== null);
      const hasNull = rawValue.length !== nonNull.length;
      const inPred = nonNull.length > 0
        ? `${col} IN (${nonNull.map((v) => quoteLiteral(v)).join(", ")})`
        : "";

      if (hasNull && inPred) predicates.push(`(${inPred} OR ${col} IS NULL)`);
      else if (hasNull) predicates.push(`${col} IS NULL`);
      else if (inPred) predicates.push(inPred);
      continue;
    }

    if (rawValue === null) {
      predicates.push(`${col} IS NULL`);
      continue;
    }

    predicates.push(`${col} = ${quoteLiteral(rawValue)}`);
  }

  return predicates.length === 0 ? "" : `WHERE ${predicates.join(" AND ")}`;
}
</file>

<file path="src/lib/duckdb/use-query.ts">
import { useCallback, useEffect, useRef, useState } from "react";

import { useDuckDB } from "./provider";

export interface DuckDBQueryResult {
  columns: string[];
  rows: unknown[][];
}

interface UseDuckDBQueryReturn {
  data: DuckDBQueryResult | null;
  loading: boolean;
  error: string | null;
  /** Re-run the query manually. */
  refetch: () => void;
}

function normalizeValue(value: unknown): unknown {
  if (value == null) return null;
  if (typeof value === "bigint") {
    const n = Number(value);
    return Number.isSafeInteger(n) ? n : value.toString();
  }
  return value;
}

/**
 * Run a SQL query against the in-browser DuckDB-WASM instance.
 *
 * The query is re-executed whenever `sql` changes (referential equality).
 * Pass `null` or `""` for `sql` to skip execution.
 */
export function useDuckDBQuery(sql: string | null): UseDuckDBQueryReturn {
  const { db, loading: dbLoading, error: dbError } = useDuckDB();

  const [data, setData] = useState<DuckDBQueryResult | null>(null);
  const [loading, setLoading] = useState(true);
  const [error, setError] = useState<string | null>(null);

  const generationRef = useRef(0);

  const execute = useCallback(async () => {
    if (!db || !sql) {
      return;
    }

    const generation = ++generationRef.current;
    setLoading(true);
    setError(null);

    let conn;
    try {
      conn = await db.connect();
      const arrowResult = await conn.query(sql);

      if (generation !== generationRef.current) return;

      const columns = arrowResult.schema.fields.map((f) => f.name);
      const rows: unknown[][] = [];

      for (let rowIdx = 0; rowIdx < arrowResult.numRows; rowIdx++) {
        const row: unknown[] = [];
        for (let colIdx = 0; colIdx < columns.length; colIdx++) {
          row.push(normalizeValue(arrowResult.getChildAt(colIdx)?.get(rowIdx)));
        }
        rows.push(row);
      }

      setData({ columns, rows });
      setLoading(false);
    } catch (err: unknown) {
      if (generation !== generationRef.current) return;
      setError(err instanceof Error ? err.message : "Query execution failed");
      setData(null);
      setLoading(false);
    } finally {
      if (conn) {
        await conn.close();
      }
    }
  }, [db, sql]);

  useEffect(() => {
    if (dbError) {
      setError(dbError);
      setLoading(false);
      return;
    }

    if (dbLoading) {
      setLoading(true);
      return;
    }

    if (!sql) {
      setData(null);
      setLoading(false);
      return;
    }

    void execute();
  }, [sql, dbLoading, dbError, execute]);

  const refetch = useCallback(() => {
    void execute();
  }, [execute]);

  return { data, loading, error, refetch };
}
</file>

<file path="repomix-notes.md">
# Repomix Notes — Excluded Content

## Excluded: design-mockups/ (4 HTML files, ~21k tokens)
Four self-contained HTML design mockups exploring different visual directions for the app:
- `01-ink-and-paper.html` — Editorial research journal aesthetic (cream, ink, Fraunces/Source Serif fonts). This was the chosen direction.
- `02-phosphor.html` — Dark/neon terminal aesthetic
- `03-velvet.html` — Rich, warm luxury aesthetic
- `04-signal.html` — Clean, minimal data-viz aesthetic

## Excluded: mcp-server/ (~5.8k tokens)
Python MCP server for AI agent access to the dataset. Key details:
- **File**: `mcp-server/server.py` (~600 lines) — FastMCP server with 5 tools: `get_schema`, `get_stats`, `cross_tabulate`, `query_data`, `search_columns`
- **Transport**: streamable-http (for Railway deployment), stdio (for local use)
- **Safety**: Read-only SQL enforcement, row limits (default 1000, max 10000), timeout guards, typed error envelopes matching API contracts
- **Deployed**: https://bks-mcp-server-production.up.railway.app (Railway Service B)
- **Dockerfile**: Installs mcp>=1.0.0 and duckdb>=1.0.0 via uv, copies data + server, exposes port 8000
- **pyproject.toml**: Minimal config with mcp and duckdb dependencies

## Excluded: test files (~7k tokens)
8 test files with 121 passing tests (Vitest):
- `contracts.test.ts` (42 tests) — Zod schema validation for all API contracts
- `sql-guards.test.ts` + `sql-guards.extended.test.ts` (42 tests) — SQL read-only enforcement, normalization, quoting
- `caveats.test.ts` + `caveats.extended.test.ts` (16 tests) — Caveat pattern matching and formatting
- `metadata.test.ts` (8 tests) — Schema metadata loading and querying
- `api-response.test.ts` (9 tests) — JSON envelope helpers
- `db.test.ts` (4 tests) — Query execution error handling

## Excluded: columns.generated.json (~26k tokens)
Auto-generated schema metadata for all 365 columns. Each entry contains:
- `name`, `logicalType` (categorical|numeric|boolean|text), `nullRatio`, `approxCardinality`
- `categoryTag` (demographic|ocean|fetish|derived|other)
- Regenerate with: `pnpm profile-schema`

## Summary: Big Kink Survey (970k cleaned).md (~45k tokens)

The Big Kink Survey is a large-scale online survey collecting data on human sexuality, kinks, fetishes, and sexual preferences. Participants were recruited primarily through Reddit, FetLife, Twitter, Facebook, and other online platforms. The survey explores relationships between demographic characteristics, childhood experiences, personality traits, and sexual interests.

The full dataset contains ~970k responses; the cleaned public subsample has 15,503 rows (stratified by age bin and biological sex) and 365 columns.

### Survey Structure (5 parts)
1. **Demographics**: gender identity, trans/cis status, HRT usage, sexual orientation, ethnicity, relationship style, location, physical characteristics, political orientation, sexual history
2. **Childhood & Upbringing**: religion, cultural attitudes, social class, locus of control, spanking/abuse history, parental configuration, Big Five personality traits
3. **Sexual Fantasy & Pornography**: emotional states in fantasy, porn consumption patterns, sex work experience, consent preferences, dom/sub orientation
4. **Vanilla Sex Acts**: oral, anal, positions, specific acts, breast size preferences, energy level preferences
5. **Fetish Categories**: hierarchical gating — broad category endorsement, then detailed follow-ups only for endorsed categories (13 "uncommon" + 17 "common" categories)

### Gating Logic
Questions are conditional on previous answers. Key gates: trans-specific questions gate on trans identity; menstrual/HRT questions gate on assigned sex; religion follow-ups gate on having a religion; abuse detail questions gate on reporting abuse; porn preference questions gate on consuming porn; each fetish category's detailed questions gate on endorsing that category. This creates substantial structural missingness — NULLs typically mean "not applicable" rather than "refused to answer."

### Derived Variables
- `biomale` (1=male, 0=female), `gendermale`, `cis`, `gendered` (binary vs nonbinary)
- Big Five composites (e.g., `opennessvariable = openness2 - openness`)
- HRT duration on ordinal scales, binary gender preference for attraction

### Data Quality Caveats
- Rating scales compressed from 0-8 to 0-5/1-5
- Questions added mid-collection create time-cohort missingness (menstrual cycle Nov 2024, horniness May 2025, sex work Oct 2024)
- Gated missingness often represents implicit zeros, not true missing data
- Some vanilla arousal scales use counterintuitive negative scoring
- Only 19 of 365 columns have zero nulls; 64% have >50% null rates (by design)
- The 19 universal columns (age, biomale, OCEAN scores, politics, etc.) are safe starting points for analysis
</file>

<file path="docs/design/architecture.md">
# Architecture

## Stack
- **Framework**: TanStack Start (React) + Nitro (server runtime)
- **Styling**: Tailwind v4
- **Build**: Vite 7, TypeScript strict mode
- **Data engine (web/API)**: DuckDB CLI (`duckdb -json`) over `data/BKSPublic.parquet`
- **Data engine (metadata generation)**: `@duckdb/node-api` in `scripts/profile-schema.mjs`
- **Data engine (MCP)**: DuckDB Python bindings in `mcp-server/server.py`
- **Deploy**: Railway (web service + optional MCP service)

## Key Technical Decisions

### Nitro plugin required for deployment
TanStack Start needs `nitro` from `nitro/vite` in `vite.config.ts` to produce `.output/` build artifacts. Without it, Vite outputs to `dist/` which does not include the Nitro server runtime.

### `verbatimModuleSyntax` disabled
TanStack docs warn that `verbatimModuleSyntax: true` can leak server bundles into client bundles. `tsconfig.json` sets it to `false`.

### Entry files
`src/router.tsx` exports `getRouter()`. `src/routeTree.gen.ts` is auto-generated by TanStack during `pnpm dev`/`pnpm build`.

### API route model
Server routes under `src/routes/api/*` are thin wrappers that:
1. Validate request boundaries via Zod contracts in `src/lib/api/contracts.ts`.
2. Apply SQL/read-only guardrails in `src/lib/server/sql-guards.ts`.
3. Execute bounded queries via `src/lib/server/db.ts`.
4. Return typed JSON envelopes (`ok`/`error`) via `src/lib/server/api-response.ts`.

### Why DuckDB CLI in the server layer
`@duckdb/node-api` is used for schema profiling scripts but currently avoided as the primary runtime engine because Nitro build/bundling with native `.node` bindings is fragile. Runtime API queries execute via the `duckdb` binary and JSON output, with timeout and row-limit controls. A fallback to `@duckdb/node-api` activates automatically when the CLI binary is not available (e.g., on Railway), with bigint normalization for JSON compatibility.

## Data and Metadata Flow
1. Source parquet lives at `data/BKSPublic.parquet`.
2. `pnpm sync-public-data` copies parquet to `public/BKSPublic.parquet`.
3. `pnpm profile-schema` regenerates `src/lib/schema/columns.generated.json`.
4. `/api/schema`, `/api/stats/$column`, `/api/crosstab`, `/api/query` consume that metadata and parquet data.
5. UI pages (`/`, `/explore`, `/profile`, `/sql`) call the API endpoints.

## Current Route Coverage
- `/` dashboard with schema stats, caveats, missingness, and per-column stats
- `/explore` cross-tab explorer with optional demographic filters
- `/profile` cohort builder with percentile summary cards
- `/sql` SQL console with result grid and CSV export
- `/api/health`, `/api/schema`, `/api/query`, `/api/stats/$column`, `/api/crosstab`
</file>

<file path="docs/design/deployment.md">
# Deployment

## Railway Project
- **Project**: bks-explorer
- **URL**: https://bks-explorer-production.up.railway.app
- **Environment**: production
- **Service**: bks-explorer (web app)

## Runtime Model
Railway auto-detects pnpm from `packageManager` in `package.json`.

Build/run pipeline:
1. `pnpm install --frozen-lockfile`
2. `pnpm build` (Vite + Nitro -> `.output/`)
3. `pnpm start` (`node .output/server/index.mjs`)

Railway injects `PORT`; Nitro uses it automatically.

## API Surface (Current)
- `GET /api/health` - health envelope
- `GET /api/schema` - dataset + column metadata + caveat definitions
- `POST /api/query` - bounded read-only SQL query
- `GET /api/stats/:column` - typed numeric/categorical summary stats
- `GET /api/crosstab` - x/y grouped counts with optional filters

## App Surface (Current)
- `GET /` dashboard
- `GET /explore` cross-tab explorer
- `GET /profile` profile/cohort percentile summary
- `GET /sql` SQL console and CSV export

## Deploying
```bash
# from repository root
railway up
```

Or via Railway MCP:
```text
mcp__Railway__deploy({ workspacePath: "/Users/austin/dev/kink" })
```

## MCP Service (Service B)
- **Service**: bks-mcp-server
- **URL**: https://bks-mcp-server-production.up.railway.app
- **MCP endpoint**: `POST https://bks-mcp-server-production.up.railway.app/mcp`
- **Transport**: streamable-http (MCP protocol over HTTP)
- **Dockerfile**: `mcp-server/Dockerfile` (build context is repo root)

Environment variables set on the service:
- `MCP_TRANSPORT=streamable-http`
- `BKS_PARQUET_PATH=/app/data/BKSPublic.parquet`
- `RAILWAY_DOCKERFILE_PATH=mcp-server/Dockerfile`
- `PORT` — injected by Railway (default 8080)

### Deploying MCP Service
```bash
# link to the MCP service first
railway service bks-mcp-server
# then deploy
railway up
```

Or via Railway MCP:
```text
mcp__Railway__link-service({ workspacePath: "...", serviceName: "bks-mcp-server" })
mcp__Railway__deploy({ workspacePath: "..." })
```

### MCP Smoke Check
```bash
curl -s -X POST \
  -H "Content-Type: application/json" \
  -H "Accept: application/json, text/event-stream" \
  -d '{"jsonrpc":"2.0","id":1,"method":"initialize","params":{"protocolVersion":"2025-03-26","capabilities":{},"clientInfo":{"name":"test","version":"1.0"}}}' \
  https://bks-mcp-server-production.up.railway.app/mcp
```

## Smoke Checks After Deploy
```bash
curl https://bks-explorer-production.up.railway.app/api/health
curl https://bks-explorer-production.up.railway.app/api/schema
curl -X POST https://bks-explorer-production.up.railway.app/api/query \
  -H 'content-type: application/json' \
  -d '{"sql":"select count(*) as n from data"}'
```

## Notes
- If production still shows older stub endpoints, redeploy from the latest commit (`railway up`).
- Local verification baseline remains: `pnpm check-types`, `pnpm test --run`, `pnpm build`.
</file>

<file path="docs/design/mcps.md">
# MCP Servers Available

## TanStack MCP (`mcp__tanstack__*`)
Access TanStack documentation, project scaffolding, and ecosystem info.

Key tools:
- `tanstack_search_docs` — Algolia search across all TanStack docs. Params: `query`, `library` (start/router/query/table), `framework` (react/vue/solid), `limit`
- `tanstack_doc` — Fetch full doc page. Params: `library`, `path` (e.g. `framework/react/guide/hosting`)
- `listTanStackAddOns` — Available add-ons for project creation
- `createTanStackApplication` — Scaffold new TanStack Start projects

Use `tanstack_search_docs` first to find the right page path, then `tanstack_doc` to fetch full content.

## Railway MCP (`mcp__Railway__*`)
Deploy and manage Railway services.

Key tools:
- `check-railway-status` — Verify CLI auth
- `create-project-and-link` — Create + link Railway project
- `deploy` — Upload and deploy from working directory
- `generate-domain` — Get a public URL
- `get-logs` — View build or deploy logs (`logType: "build"` or `"deploy"`)
- `list-services` — See services in linked project
- `set-variables` — Set env vars
- `link-service` / `link-environment` — Switch service/env context

## PostgreSQL/TimescaleDB MCP (`mcp__plugin_pg_pg-aiguide__*`)
- `search_docs` — Search Tiger Cloud or PostgreSQL docs

## Chrome MCP (`mcp__claude-in-chrome__*`)
Browser automation — navigate, click, read pages, take screenshots, run JS.

## BKS MCP Server (deployed)
Python MCP server at `mcp-server/server.py` for AI agent access to BKS data.

- **URL**: https://bks-mcp-server-production.up.railway.app
- **Endpoint**: `POST /mcp` (streamable-http transport)
- **Railway service**: `bks-mcp-server`

Tools:
- `get_schema(timeout_ms?)`
- `get_stats(column, top_n?, timeout_ms?)`
- `cross_tabulate(x_column, y_column, top_n?, include_nulls?, timeout_ms?)`
- `query_data(sql, limit?, timeout_ms?)`
- `search_columns(query, limit?)`

Behavior parity with plan/API conventions:
- Typed envelopes on every tool call:
  - success: `{ ok: true, data, meta? }`
  - error: `{ ok: false, error: { code, message, details? } }`
- Read-only SQL guardrails for `query_data`:
  - only `SELECT`, `WITH`, `DESCRIBE`, `EXPLAIN` statement types
  - mutating keywords blocked (`INSERT`, `UPDATE`, `DELETE`, `DROP`, etc.)
  - single-statement only (no chained statements via `;`)
- Bounded results:
  - default row limit `1000`
  - hard max row limit `10000`
- Timeout handling:
  - configurable `timeout_ms` (default `5000`, capped at `30000`)
  - server attempts DuckDB `statement_timeout`; response metadata reports whether timeout was enforced

Uses DuckDB Python bindings with in-memory DuckDB reading from a bundled parquet file.

Supports two transports:
- `stdio` (default) — for local use via `python server.py`
- `streamable-http` — set `MCP_TRANSPORT=streamable-http` for HTTP deployment; reads `PORT` env var
</file>

<file path="docs/plans/completed/2026-02-12-plan-continuation.md">
# Plan Continuation - 2026-02-12

Owner: Codex agent session
Source plans: `PLAN.md`, `META-PLAN.md`

## Objective
Advance the repo from scaffold to a fully usable local application with hardened APIs, functional UI routes, and aligned MCP tooling.

## Milestone Checklist
- [x] Establish active execution plan and progress log.
- [x] Implement schema profiling + shared contracts/caveats.
- [x] Build hardened DuckDB-backed API routes (`schema`, `query`, `stats`, `crosstab`).
- [x] Implement interactive dashboard/explore/profile/sql pages using API contracts.
- [x] Upgrade MCP tools to parity with API behavior and safety guardrails.
- [x] Add tests for contract + SQL guardrails + data helpers.
- [x] Validate (`check-types`, `test --run`, `build`) and fix regressions.
- [x] Move this file to `docs/plans/completed/` with closure summary.

## Progress Log
- 2026-02-12: Scanned repository baseline and plan docs.
- 2026-02-12: Confirmed baseline state: scaffold UI + stub APIs + basic MCP server.
- 2026-02-12: Baseline validation run: `pnpm check-types` pass, `pnpm build` pass, `pnpm test --run` fails (no tests).
- 2026-02-12: Started implementation sequence.
- 2026-02-12: Added data scripts: `scripts/sync-public-data.mjs`, `scripts/profile-schema.mjs`.
- 2026-02-12: Generated `src/lib/schema/columns.generated.json` from parquet (365 columns).
- 2026-02-12: Added shared schema + caveat modules and API contracts.
- 2026-02-12: Replaced API stubs with DuckDB-backed routes (`/api/schema`, `/api/query`, `/api/stats/$column`, `/api/crosstab`) and SQL guardrails.
- 2026-02-12: Added unit tests for SQL guardrails and caveat mapping.
- 2026-02-12: Smoke-tested API endpoints locally via `curl` against `pnpm dev`.
- 2026-02-12: Upgraded `mcp-server/server.py` tools to typed envelope + guardrail behavior parity.
- 2026-02-12: Implemented non-placeholder UI flows for `/`, `/explore`, `/profile`, `/sql`.
- 2026-02-12: Updated architecture, deployment, MCP, and schema docs to reflect current implementation.
- 2026-02-12: Validation reruns: `pnpm check-types` pass, `pnpm build` pass, `pnpm test --run` pass (Vitest reports lingering file handles in this environment after completion).

## Notes
- Keep API responses in standard envelope format.
- Keep route handlers thin by pushing SQL logic into shared server libs.
- Surface structural missingness and column caveats in schema responses.

## Closure Summary
- Core milestones M0-M5 are now implemented in local code and validated.
- Deployment (M6) documentation is updated; production redeploy is still a separate operational step.
</file>

<file path="docs/schema/README.md">
# Schema Metadata

## Source of Truth
- Generated metadata: `src/lib/schema/columns.generated.json`
- Shared TypeScript types: `src/lib/schema/types.ts`
- Column caveat mapping: `src/lib/schema/caveats.ts`

## Regeneration
Run both commands from repo root:
```bash
pnpm sync-public-data
pnpm profile-schema
```

`profile-schema` computes, per column:
- name
- DuckDB type
- inferred logical type (`categorical|numeric|boolean|text|unknown`)
- null ratio
- approximate cardinality
- category tags (`demographic|ocean|fetish|derived|other`)

## Caveat Model
Two caveats are global across analysis:
- gated missingness
- late-added questions

Additional per-column caveats are pattern-mapped for known modified fields from `BKSPublic_column_notes.txt`:
- binned/collapsed
- combined/merged
- computed

## API Exposure
`GET /api/schema` returns:
- dataset metadata
- per-column metadata + caveat keys
- caveat definitions for UI/agent rendering

## Notes
- This metadata is intended for UI controls, API validation context, and MCP tool assistive hints.
- It is not a replacement for the original survey wording document in `data/Big Kink Survey (970k cleaned).md`.
</file>

<file path="docs/worklog.md">
# Worklog

## 2026-02-12

### Session 2 - Parallel agent buildout

**DuckDB-WASM client-side queries**: Adding browser-side DuckDB-WASM so UI pages can query parquet directly without server round-trips. In progress.

**MCP Service B deployment**: Deploying the Python MCP server (`mcp-server/`) as a separate Railway service (Service B) for AI agent access. In progress.

**Comprehensive tests**: Adding unit and integration tests for SQL guardrails, API contracts, schema utilities, and data helpers. In progress.

**Pre-commit hooks**: Setting up husky + lint-staged to enforce lint, type-check, and test gates before every commit.

**Open decisions resolved** (PLAN.md section 13):
- Public API: broadly available with read-only guardrails
- SQL console: production, no feature flag
- MCP service: internet-exposed, no auth restriction

**Data exploration summary**: Deep exploration of the BKS dataset (15.5k rows, 365 columns) to document distributions, notable patterns, and data quality characteristics.

---

### 22:13 UTC - Deployment verification and limitation discovered
- **Status**: Railway deployment `ee1487d9-ced4-40f3-822f-0fcf0266f37f` reached `SUCCESS`.
- **Observed limitation**:
  - Production routes that execute SQL (`/api/query`, `/api/stats/:column`, `/api/crosstab`) return `400 QUERY_EXECUTION_FAILED`.
  - Non-query routes (`/`, `/explore`, `/profile`, `/sql`, `/api/health`, `/api/schema`) return `200`.
- **Likely cause**:
  - Server runtime currently depends on invoking the `duckdb` CLI binary via child process.
  - Railway runtime environment likely does not include the `duckdb` CLI executable in `PATH`.
- **Workaround plan in progress**:
  - Add runtime fallback in server DB layer to use `@duckdb/node-api` when CLI execution fails with executable-not-found.
  - Keep CLI path as first choice for local/dev compatibility.
  - Redeploy and re-run production smoke checks.
- **Next step**: implement fallback in `src/lib/server/db.ts` and redeploy.

### 22:15 UTC - Runtime fallback implemented and validated locally
- **Change**: `src/lib/server/db.ts` now attempts `duckdb` CLI first, then falls back to `@duckdb/node-api` if the CLI executable is missing (`ENOENT`).
- **Reason**: Railway runtime does not guarantee presence of `duckdb` CLI binary.
- **Validation**:
  - `pnpm check-types` pass.
  - `pnpm build` pass.
  - Simulated no-CLI environment (`PATH` without duckdb) and confirmed `/api/query` still returns `200` via Node API fallback.
- **Current status**: ready to redeploy to Railway and re-run production smoke checks.

### 22:16 UTC - Redeploy started
- **Deployment id**: `5211c45e-1325-44ff-8963-a1f95259cb4e`
- **Objective**: verify that SQL API routes recover on Railway with runtime fallback.
- **Next checks**: `/api/query`, `/api/stats/straightness`, `/api/crosstab?...` on production URL after deploy success.
- **Status update**: deployment currently `BUILDING`.
- **Status update**: deployment progressed to `DEPLOYING`.
- **Status update**: deployment reached `SUCCESS`.

### 22:18 UTC - Post-deploy regression identified and fixed
- **Observed after first fallback deploy**:
  - `/api/query` returned `500 QUERY_FAILED`.
  - `/api/stats/:column` returned `200` but with zero count fields.
- **Root cause**:
  - `@duckdb/node-api` path returns `bigint` values for aggregates.
  - `bigint` values were not normalized before JSON serialization/number parsing.
- **Workaround/fix**:
  - Added `bigint` normalization in `src/lib/server/db.ts` (`safe integer -> number`, otherwise `string`).
- **Validation in no-CLI simulation**:
  - `/api/query` -> `200` with expected rows.
  - `/api/stats/straightness` -> `200` with non-zero counts.
  - `/api/crosstab?...` -> `200` with expected counts.
- **Status**: fix validated locally; deploying updated build to Railway next.

### 22:19 UTC - Second redeploy started
- **Deployment id**: `d30133e6-8b77-4135-84f7-12451ba0f0b0`
- **Purpose**: ship bigint-normalization fix for Node API fallback path.
- **Status update**: second redeploy currently `BUILDING`.
- **Status update**: second redeploy progressed to `DEPLOYING`.
- **Status update**: second redeploy reached `SUCCESS`.

### 22:19 UTC - Production smoke checks after second redeploy
- **Production URL**: `https://bks-explorer-production.up.railway.app`
- **Smoke results**:
  - `GET /api/health` -> `200`
  - `GET /api/schema` -> `200`
  - `GET /api/stats/straightness` -> `200` with expected non-zero counts
  - `GET /api/crosstab?x=straightness&y=politics&limit=3` -> `200`
  - `POST /api/query` -> `200`
- **Current status**: deployment healthy and query endpoints operational in production.
- **Known limitation (tracked)**:
  - Runtime uses dual-path query execution (CLI first, Node API fallback). This is intentional until hosting guarantees include DuckDB CLI binary.
</file>

<file path="mcp-server/Dockerfile">
FROM python:3.12-slim

WORKDIR /app

COPY --from=ghcr.io/astral-sh/uv:latest /uv /usr/local/bin/uv

RUN uv pip install --system "mcp>=1.0.0" "duckdb>=1.0.0"

COPY data/BKSPublic.parquet /app/data/BKSPublic.parquet
COPY mcp-server/server.py .

ENV BKS_PARQUET_PATH=/app/data/BKSPublic.parquet
ENV MCP_TRANSPORT=streamable-http

EXPOSE 8000

CMD ["python", "server.py"]
</file>

<file path="mcp-server/pyproject.toml">
[project]
name = "bks-mcp-server"
version = "0.1.0"
description = "MCP server for the Big Kink Survey dataset"
requires-python = ">=3.11"
dependencies = [
    "mcp>=1.0.0",
    "duckdb>=1.0.0",
]

[build-system]
requires = ["hatchling"]
build-backend = "hatchling.build"
</file>

<file path="mcp-server/server.py">
"""MCP server for the Big Kink Survey dataset."""

from __future__ import annotations

import os
import re
import time
from datetime import date, datetime, time as datetime_time
from decimal import Decimal
from pathlib import Path
from typing import Any

import duckdb
from duckdb import DuckDBPyConnection
from mcp.server.fastmcp import FastMCP

PARQUET_PATH = os.environ.get(
    "BKS_PARQUET_PATH",
    str(Path(__file__).parent.parent / "data" / "BKSPublic.parquet"),
)
DATA_TABLE = "data"

DEFAULT_LIMIT = int(os.environ.get("BKS_QUERY_DEFAULT_LIMIT", "1000"))
MAX_LIMIT = int(os.environ.get("BKS_QUERY_MAX_LIMIT", "10000"))
DEFAULT_TIMEOUT_MS = int(os.environ.get("BKS_QUERY_TIMEOUT_MS", "5000"))
MAX_TIMEOUT_MS = int(os.environ.get("BKS_QUERY_MAX_TIMEOUT_MS", "30000"))

DEFAULT_TOP_N = int(os.environ.get("BKS_TOP_N_DEFAULT", "20"))
MAX_TOP_N = int(os.environ.get("BKS_TOP_N_MAX", "100"))
NULL_LABEL = "<NULL>"

READ_ONLY_PREFIXES = {"SELECT", "WITH", "DESCRIBE", "EXPLAIN"}
MUTATING_KEYWORDS_RE = re.compile(
    r"\b(INSERT|UPDATE|DELETE|DROP|ALTER|CREATE|COPY|ATTACH|DETACH|INSTALL|LOAD|"
    r"CALL|TRUNCATE|VACUUM|MERGE|REPLACE|GRANT|REVOKE|COMMENT|ANALYZE)\b",
    flags=re.IGNORECASE,
)
NUMERIC_TYPE_PREFIXES = (
    "TINYINT",
    "SMALLINT",
    "INTEGER",
    "BIGINT",
    "HUGEINT",
    "UTINYINT",
    "USMALLINT",
    "UINTEGER",
    "UBIGINT",
    "FLOAT",
    "DOUBLE",
    "REAL",
    "DECIMAL",
    "NUMERIC",
)

_transport = os.environ.get("MCP_TRANSPORT", "stdio")
_mcp_kwargs: dict[str, Any] = {}
if _transport == "streamable-http":
    _mcp_kwargs["host"] = "0.0.0.0"
    _mcp_kwargs["port"] = int(os.environ.get("PORT", "8000"))

mcp = FastMCP("Big Kink Survey", **_mcp_kwargs)


def success(data: Any, meta: dict[str, Any] | None = None) -> dict[str, Any]:
    payload: dict[str, Any] = {"ok": True, "data": data}
    if meta is not None:
        payload["meta"] = meta
    return payload


def failure(code: str, message: str, details: Any | None = None) -> dict[str, Any]:
    error: dict[str, Any] = {"code": code, "message": message}
    if details is not None:
        error["details"] = details
    return {"ok": False, "error": error}


def normalize_limit(
    limit: int | None,
    *,
    default: int = DEFAULT_LIMIT,
    maximum: int = MAX_LIMIT,
) -> int:
    if limit is None:
        limit = default
    try:
        limit_int = int(limit)
    except (TypeError, ValueError):
        limit_int = default
    return max(1, min(limit_int, maximum))


def normalize_timeout_ms(timeout_ms: int | None) -> int:
    if timeout_ms is None:
        timeout_ms = DEFAULT_TIMEOUT_MS
    try:
        timeout_int = int(timeout_ms)
    except (TypeError, ValueError):
        timeout_int = DEFAULT_TIMEOUT_MS
    return max(0, min(timeout_int, MAX_TIMEOUT_MS))


def normalize_top_n(top_n: int | None, *, default: int = DEFAULT_TOP_N) -> int:
    if top_n is None:
        top_n = default
    try:
        top_n_int = int(top_n)
    except (TypeError, ValueError):
        top_n_int = default
    return max(1, min(top_n_int, MAX_TOP_N))


def quote_ident(identifier: str) -> str:
    return '"' + identifier.replace('"', '""') + '"'


def statement_type(sql: str) -> str:
    match = re.match(r"^([A-Za-z]+)", sql)
    if not match:
        return ""
    return match.group(1).upper()


def validate_read_only_sql(sql: str) -> tuple[str | None, str | None]:
    cleaned = sql.strip()
    if not cleaned:
        return None, "sql field is required"

    while cleaned.endswith(";"):
        cleaned = cleaned[:-1].rstrip()

    if ";" in cleaned:
        return None, "Only a single SQL statement is allowed"

    kind = statement_type(cleaned)
    if kind not in READ_ONLY_PREFIXES:
        allowed = ", ".join(sorted(READ_ONLY_PREFIXES))
        return None, f"Only read-only queries are allowed ({allowed})"

    if MUTATING_KEYWORDS_RE.search(cleaned):
        return None, "Mutating SQL keywords are not allowed"

    return cleaned, None


def is_numeric_type(db_type: str) -> bool:
    upper = db_type.upper()
    return upper.startswith(NUMERIC_TYPE_PREFIXES)


def is_timeout_error(exc: Exception) -> bool:
    message = str(exc).lower()
    return "timeout" in message or "timed out" in message


def to_json_value(value: Any) -> Any:
    if isinstance(value, (datetime, date, datetime_time)):
        return value.isoformat()
    if isinstance(value, Decimal):
        return float(value)
    if isinstance(value, bytes):
        return value.decode("utf-8", errors="replace")
    if isinstance(value, list):
        return [to_json_value(item) for item in value]
    if isinstance(value, tuple):
        return [to_json_value(item) for item in value]
    if isinstance(value, dict):
        return {str(key): to_json_value(item) for key, item in value.items()}
    return value


def with_timeout(conn: DuckDBPyConnection, timeout_ms: int) -> bool:
    if timeout_ms <= 0:
        return False
    try:
        conn.execute(f"SET statement_timeout='{timeout_ms}ms'")
        return True
    except Exception:
        return False


def get_connection(timeout_ms: int = DEFAULT_TIMEOUT_MS) -> tuple[DuckDBPyConnection, bool]:
    dataset_path = Path(PARQUET_PATH)
    if not dataset_path.exists():
        raise FileNotFoundError(str(dataset_path))

    conn = duckdb.connect(":memory:")
    escaped_path = str(dataset_path).replace("'", "''")
    conn.execute(
        f"CREATE OR REPLACE VIEW {DATA_TABLE} AS "
        f"SELECT * FROM read_parquet('{escaped_path}')"
    )
    timeout_enforced = with_timeout(conn, timeout_ms)
    return conn, timeout_enforced


def describe_columns(conn: DuckDBPyConnection) -> list[dict[str, Any]]:
    rows = conn.execute(f"DESCRIBE {DATA_TABLE}").fetchall()
    return [
        {"name": row[0], "type": row[1], "nullable": str(row[2]).upper() == "YES"}
        for row in rows
    ]


def resolve_column_name(column: str, columns: list[dict[str, Any]]) -> str:
    names = [item["name"] for item in columns]
    if column in names:
        return column

    lowered = column.lower()
    case_insensitive = [name for name in names if name.lower() == lowered]
    if len(case_insensitive) == 1:
        return case_insensitive[0]
    if len(case_insensitive) > 1:
        raise ValueError(
            f"Column '{column}' is ambiguous; use exact casing for one of: "
            f"{', '.join(case_insensitive)}"
        )
    raise KeyError(column)


@mcp.tool()
def get_schema(timeout_ms: int = DEFAULT_TIMEOUT_MS) -> dict[str, Any]:
    """Get dataset schema and row metadata."""
    timeout = normalize_timeout_ms(timeout_ms)
    conn: DuckDBPyConnection | None = None
    try:
        conn, timeout_enforced = get_connection(timeout)
        columns = describe_columns(conn)
        row_count = int(
            conn.execute(f"SELECT COUNT(*)::BIGINT FROM {DATA_TABLE}").fetchone()[0]
        )
        data = {
            "rowCount": row_count,
            "columnCount": len(columns),
            "columns": columns,
        }
        meta = {
            "datasetPath": str(Path(PARQUET_PATH)),
            "timeoutMs": timeout,
            "timeoutEnforced": timeout_enforced,
        }
        return success(data, meta)
    except FileNotFoundError:
        return failure(
            "DATASET_NOT_FOUND",
            "Dataset parquet file was not found.",
            {"path": str(Path(PARQUET_PATH))},
        )
    except Exception as exc:
        code = "QUERY_TIMEOUT" if is_timeout_error(exc) else "SCHEMA_QUERY_FAILED"
        message = (
            "Schema request exceeded the configured timeout."
            if code == "QUERY_TIMEOUT"
            else "Failed to read dataset schema."
        )
        return failure(code, message, {"reason": str(exc)})
    finally:
        if conn is not None:
            conn.close()


@mcp.tool()
def query_data(
    sql: str,
    limit: int = DEFAULT_LIMIT,
    timeout_ms: int = DEFAULT_TIMEOUT_MS,
) -> dict[str, Any]:
    """Run a bounded read-only SQL query against the BKS dataset (table name: data)."""
    if not isinstance(sql, str) or not sql.strip():
        return failure("MISSING_SQL", "sql field is required")

    cleaned, sql_error = validate_read_only_sql(sql)
    if sql_error:
        return failure("UNSAFE_SQL", sql_error)
    if cleaned is None:
        return failure("UNSAFE_SQL", "Invalid SQL query")

    bounded_limit = normalize_limit(limit)
    timeout = normalize_timeout_ms(timeout_ms)
    stmt_type = statement_type(cleaned)

    conn: DuckDBPyConnection | None = None
    start = time.perf_counter()
    try:
        conn, timeout_enforced = get_connection(timeout)
        if stmt_type in {"SELECT", "WITH"}:
            bounded_sql = (
                f"SELECT * FROM ({cleaned}) AS _bks_query_result LIMIT {bounded_limit}"
            )
        else:
            bounded_sql = cleaned

        result = conn.execute(bounded_sql)
        columns = [desc[0] for desc in (result.description or [])]
        raw_rows = result.fetchall()
        rows = [[to_json_value(item) for item in row] for row in raw_rows]

        may_be_truncated = stmt_type in {"SELECT", "WITH"} and len(rows) == bounded_limit
        if stmt_type not in {"SELECT", "WITH"} and len(rows) > bounded_limit:
            rows = rows[:bounded_limit]

        elapsed_ms = round((time.perf_counter() - start) * 1000, 2)
        return success(
            {"columns": columns, "rows": rows},
            {
                "limit": bounded_limit,
                "rowCount": len(rows),
                "statementType": stmt_type,
                "timeoutMs": timeout,
                "timeoutEnforced": timeout_enforced,
                "durationMs": elapsed_ms,
                "mayBeTruncated": may_be_truncated,
            },
        )
    except FileNotFoundError:
        return failure(
            "DATASET_NOT_FOUND",
            "Dataset parquet file was not found.",
            {"path": str(Path(PARQUET_PATH))},
        )
    except Exception as exc:
        code = "QUERY_TIMEOUT" if is_timeout_error(exc) else "QUERY_FAILED"
        message = (
            "Query exceeded the configured timeout."
            if code == "QUERY_TIMEOUT"
            else "Failed to execute SQL query."
        )
        return failure(code, message, {"reason": str(exc)})
    finally:
        if conn is not None:
            conn.close()


@mcp.tool()
def get_stats(
    column: str,
    top_n: int = 10,
    timeout_ms: int = DEFAULT_TIMEOUT_MS,
) -> dict[str, Any]:
    """Get typed summary statistics for one column."""
    if not isinstance(column, str) or not column.strip():
        return failure("MISSING_COLUMN", "column is required")

    requested_column = column.strip()
    bounded_top_n = normalize_top_n(top_n, default=10)
    timeout = normalize_timeout_ms(timeout_ms)

    conn: DuckDBPyConnection | None = None
    try:
        conn, timeout_enforced = get_connection(timeout)
        columns = describe_columns(conn)
        try:
            column_name = resolve_column_name(requested_column, columns)
        except KeyError:
            return failure(
                "COLUMN_NOT_FOUND",
                f"Column '{requested_column}' was not found.",
            )
        except ValueError as exc:
            return failure("AMBIGUOUS_COLUMN", str(exc))

        column_info = next(item for item in columns if item["name"] == column_name)
        column_ident = quote_ident(column_name)

        totals = conn.execute(
            f"SELECT COUNT(*)::BIGINT AS total, "
            f"COUNT({column_ident})::BIGINT AS non_null "
            f"FROM {DATA_TABLE}"
        ).fetchone()
        total = int(totals[0])
        non_null = int(totals[1])
        nulls = total - non_null
        null_ratio = round((nulls / total), 6) if total else 0.0

        if is_numeric_type(column_info["type"]):
            numeric_row = conn.execute(
                f"SELECT AVG({column_ident}) AS mean, "
                f"STDDEV_SAMP({column_ident}) AS stddev, "
                f"MIN({column_ident}) AS min, "
                f"quantile_cont({column_ident}, 0.25) AS p25, "
                f"quantile_cont({column_ident}, 0.5) AS median, "
                f"quantile_cont({column_ident}, 0.75) AS p75, "
                f"MAX({column_ident}) AS max "
                f"FROM {DATA_TABLE} "
                f"WHERE {column_ident} IS NOT NULL"
            ).fetchone()
            stats = {
                "mean": to_json_value(numeric_row[0]),
                "stddev": to_json_value(numeric_row[1]),
                "min": to_json_value(numeric_row[2]),
                "p25": to_json_value(numeric_row[3]),
                "median": to_json_value(numeric_row[4]),
                "p75": to_json_value(numeric_row[5]),
                "max": to_json_value(numeric_row[6]),
            }
            logical_type = "numeric"
        else:
            distinct_count = int(
                conn.execute(
                    f"SELECT COUNT(DISTINCT {column_ident})::BIGINT "
                    f"FROM {DATA_TABLE} "
                    f"WHERE {column_ident} IS NOT NULL"
                ).fetchone()[0]
            )
            top_rows = conn.execute(
                f"SELECT CAST({column_ident} AS VARCHAR) AS value, "
                f"COUNT(*)::BIGINT AS count "
                f"FROM {DATA_TABLE} "
                f"WHERE {column_ident} IS NOT NULL "
                f"GROUP BY 1 "
                f"ORDER BY count DESC, value ASC "
                f"LIMIT ?",
                [bounded_top_n],
            ).fetchall()

            top_categories = []
            for value, count in top_rows:
                count_int = int(count)
                top_categories.append(
                    {
                        "value": to_json_value(value),
                        "count": count_int,
                        "percent": round((count_int / non_null) * 100, 4)
                        if non_null
                        else 0.0,
                    }
                )

            stats = {
                "distinctCount": distinct_count,
                "topCategories": top_categories,
                "topN": bounded_top_n,
                "truncated": distinct_count > len(top_categories),
            }
            logical_type = "categorical"

        return success(
            {
                "column": column_name,
                "columnType": column_info["type"],
                "logicalType": logical_type,
                "totalCount": total,
                "nonNullCount": non_null,
                "nullCount": nulls,
                "nullRatio": null_ratio,
                "stats": stats,
            },
            {
                "timeoutMs": timeout,
                "timeoutEnforced": timeout_enforced,
            },
        )
    except FileNotFoundError:
        return failure(
            "DATASET_NOT_FOUND",
            "Dataset parquet file was not found.",
            {"path": str(Path(PARQUET_PATH))},
        )
    except Exception as exc:
        code = "QUERY_TIMEOUT" if is_timeout_error(exc) else "STATS_QUERY_FAILED"
        message = (
            "Stats request exceeded the configured timeout."
            if code == "QUERY_TIMEOUT"
            else "Failed to compute column statistics."
        )
        return failure(code, message, {"reason": str(exc)})
    finally:
        if conn is not None:
            conn.close()


@mcp.tool()
def cross_tabulate(
    x_column: str,
    y_column: str,
    top_n: int = DEFAULT_TOP_N,
    include_nulls: bool = False,
    timeout_ms: int = DEFAULT_TIMEOUT_MS,
) -> dict[str, Any]:
    """Build a cross-tab matrix with marginal totals for two columns."""
    if not isinstance(x_column, str) or not x_column.strip():
        return failure("MISSING_X_COLUMN", "x_column is required")
    if not isinstance(y_column, str) or not y_column.strip():
        return failure("MISSING_Y_COLUMN", "y_column is required")

    requested_x = x_column.strip()
    requested_y = y_column.strip()
    bounded_top_n = normalize_top_n(top_n)
    timeout = normalize_timeout_ms(timeout_ms)

    conn: DuckDBPyConnection | None = None
    try:
        conn, timeout_enforced = get_connection(timeout)
        columns = describe_columns(conn)

        try:
            x_name = resolve_column_name(requested_x, columns)
        except KeyError:
            return failure("COLUMN_NOT_FOUND", f"Column '{requested_x}' was not found.")
        except ValueError as exc:
            return failure("AMBIGUOUS_COLUMN", str(exc))

        try:
            y_name = resolve_column_name(requested_y, columns)
        except KeyError:
            return failure("COLUMN_NOT_FOUND", f"Column '{requested_y}' was not found.")
        except ValueError as exc:
            return failure("AMBIGUOUS_COLUMN", str(exc))

        x_ident = quote_ident(x_name)
        y_ident = quote_ident(y_name)

        if include_nulls:
            x_expr = f"COALESCE(CAST({x_ident} AS VARCHAR), '{NULL_LABEL}')"
            y_expr = f"COALESCE(CAST({y_ident} AS VARCHAR), '{NULL_LABEL}')"
            where_clause = ""
        else:
            x_expr = f"CAST({x_ident} AS VARCHAR)"
            y_expr = f"CAST({y_ident} AS VARCHAR)"
            where_clause = f"WHERE {x_ident} IS NOT NULL AND {y_ident} IS NOT NULL"

        base_sql = (
            f"SELECT {x_expr} AS x_value, {y_expr} AS y_value "
            f"FROM {DATA_TABLE} {where_clause}"
        )

        distinct_row = conn.execute(
            "WITH base AS (" + base_sql + ") "
            "SELECT COUNT(DISTINCT x_value)::BIGINT, "
            "COUNT(DISTINCT y_value)::BIGINT, "
            "COUNT(*)::BIGINT FROM base"
        ).fetchone()
        x_distinct = int(distinct_row[0])
        y_distinct = int(distinct_row[1])
        base_row_count = int(distinct_row[2])

        x_rows = conn.execute(
            "WITH base AS (" + base_sql + ") "
            "SELECT x_value, COUNT(*)::BIGINT AS count "
            "FROM base GROUP BY 1 "
            "ORDER BY count DESC, x_value ASC "
            "LIMIT ?",
            [bounded_top_n],
        ).fetchall()
        y_rows = conn.execute(
            "WITH base AS (" + base_sql + ") "
            "SELECT y_value, COUNT(*)::BIGINT AS count "
            "FROM base GROUP BY 1 "
            "ORDER BY count DESC, y_value ASC "
            "LIMIT ?",
            [bounded_top_n],
        ).fetchall()

        x_values = [to_json_value(row[0]) for row in x_rows]
        y_values = [to_json_value(row[0]) for row in y_rows]
        x_set = set(x_values)
        y_set = set(y_values)

        pair_rows = conn.execute(
            "WITH base AS (" + base_sql + ") "
            "SELECT x_value, y_value, COUNT(*)::BIGINT AS count "
            "FROM base GROUP BY 1, 2"
        ).fetchall()

        cell_counts: dict[tuple[Any, Any], int] = {}
        row_totals: dict[Any, int] = {value: 0 for value in x_values}
        column_totals: dict[Any, int] = {value: 0 for value in y_values}

        for x_value_raw, y_value_raw, count_raw in pair_rows:
            x_value = to_json_value(x_value_raw)
            y_value = to_json_value(y_value_raw)
            if x_value not in x_set or y_value not in y_set:
                continue
            count_int = int(count_raw)
            cell_counts[(x_value, y_value)] = count_int
            row_totals[x_value] += count_int
            column_totals[y_value] += count_int

        matrix = [
            [cell_counts.get((x_value, y_value), 0) for y_value in y_values]
            for x_value in x_values
        ]
        cells = [
            {"x": x_value, "y": y_value, "count": cell_counts[(x_value, y_value)]}
            for x_value in x_values
            for y_value in y_values
            if (x_value, y_value) in cell_counts
        ]
        row_totals_list = [
            {"x": x_value, "count": row_totals[x_value]} for x_value in x_values
        ]
        column_totals_list = [
            {"y": y_value, "count": column_totals[y_value]} for y_value in y_values
        ]
        grand_total = sum(row_totals.values())

        return success(
            {
                "xColumn": x_name,
                "yColumn": y_name,
                "xValues": x_values,
                "yValues": y_values,
                "matrix": matrix,
                "cells": cells,
                "rowTotals": row_totals_list,
                "columnTotals": column_totals_list,
                "grandTotal": grand_total,
                "baseRowCount": base_row_count,
            },
            {
                "topN": bounded_top_n,
                "includeNulls": include_nulls,
                "xDistinctCount": x_distinct,
                "yDistinctCount": y_distinct,
                "xTruncated": x_distinct > len(x_values),
                "yTruncated": y_distinct > len(y_values),
                "timeoutMs": timeout,
                "timeoutEnforced": timeout_enforced,
            },
        )
    except FileNotFoundError:
        return failure(
            "DATASET_NOT_FOUND",
            "Dataset parquet file was not found.",
            {"path": str(Path(PARQUET_PATH))},
        )
    except Exception as exc:
        code = "QUERY_TIMEOUT" if is_timeout_error(exc) else "CROSSTAB_QUERY_FAILED"
        message = (
            "Cross-tab request exceeded the configured timeout."
            if code == "QUERY_TIMEOUT"
            else "Failed to compute cross-tabulation."
        )
        return failure(code, message, {"reason": str(exc)})
    finally:
        if conn is not None:
            conn.close()


@mcp.tool()
def search_columns(query: str, limit: int = 25) -> dict[str, Any]:
    """Search for columns by case-insensitive name match."""
    if not isinstance(query, str) or not query.strip():
        return failure("MISSING_QUERY", "query is required")

    term = query.strip().lower()
    bounded_limit = normalize_limit(limit, default=25, maximum=250)

    conn: DuckDBPyConnection | None = None
    try:
        conn, timeout_enforced = get_connection(DEFAULT_TIMEOUT_MS)
        columns = describe_columns(conn)

        ranked: list[tuple[int, str, dict[str, Any]]] = []
        for column in columns:
            name = column["name"]
            lowered = name.lower()
            if term not in lowered:
                continue
            if lowered == term:
                score = 0
            elif lowered.startswith(term):
                score = 1
            else:
                score = 2
            ranked.append((score, lowered, column))

        ranked.sort(key=lambda item: (item[0], item[1]))
        matches = [
            {
                "name": item["name"],
                "type": item["type"],
                "nullable": item["nullable"],
            }
            for _, _, item in ranked[:bounded_limit]
        ]

        return success(
            {
                "query": query,
                "matchCount": len(matches),
                "totalMatches": len(ranked),
                "matches": matches,
            },
            {
                "limit": bounded_limit,
                "timeoutMs": DEFAULT_TIMEOUT_MS,
                "timeoutEnforced": timeout_enforced,
            },
        )
    except FileNotFoundError:
        return failure(
            "DATASET_NOT_FOUND",
            "Dataset parquet file was not found.",
            {"path": str(Path(PARQUET_PATH))},
        )
    except Exception as exc:
        code = "QUERY_TIMEOUT" if is_timeout_error(exc) else "SEARCH_QUERY_FAILED"
        message = (
            "Column search exceeded the configured timeout."
            if code == "QUERY_TIMEOUT"
            else "Failed to search columns."
        )
        return failure(code, message, {"reason": str(exc)})
    finally:
        if conn is not None:
            conn.close()


if __name__ == "__main__":
    mcp.run(transport=_transport)
</file>

<file path="scripts/profile-schema.mjs">
#!/usr/bin/env node

import { mkdir, stat, writeFile } from "node:fs/promises";
import { dirname, resolve } from "node:path";
import { fileURLToPath } from "node:url";

import { DuckDBInstance } from "@duckdb/node-api";

const __dirname = dirname(fileURLToPath(import.meta.url));
const projectRoot = resolve(__dirname, "..");

const sourcePath = process.env.BKS_PARQUET_PATH ?? resolve(projectRoot, "data", "BKSPublic.parquet");
const outputPath = process.env.BKS_COLUMNS_OUTPUT ?? resolve(projectRoot, "src", "lib", "schema", "columns.generated.json");

const derivedColumns = new Set([
  "straightness",
  "childhood_adversity",
  "childhood_gender_tolerance",
  "TotalMentalIllness",
  "opennessvariable",
  "consciensiousnessvariable",
  "extroversionvariable",
  "neuroticismvariable",
  "agreeablenessvariable",
  "powerlessnessvariable",
  "totalfetishcategory",
  "bondageaverage",
]);

const demographicHints = [
  "age",
  "gender",
  "male",
  "female",
  "cis",
  "trans",
  "politics",
  "bmi",
  "relationship",
  "education",
  "income",
  "childhood",
  "straightness",
  "orientation",
  "liberated",
];

const fetishHints = [
  "fetish",
  "bondage",
  "nonconsent",
  "sadism",
  "masoch",
  "submission",
  "dominant",
  "kink",
  "erotic",
  "sexual",
  "voyeur",
  "exhibition",
  "humiliation",
  "transformation",
];

function quoteIdentifier(identifier) {
  return `"${identifier.replaceAll('"', '""')}"`;
}

function isNumericDuckType(type) {
  return /(TINYINT|SMALLINT|INTEGER|BIGINT|HUGEINT|UTINYINT|USMALLINT|UINTEGER|UBIGINT|FLOAT|DOUBLE|DECIMAL|REAL)/i.test(type);
}

function inferLogicalType(name, duckdbType, approxCardinality) {
  const lowerName = name.toLowerCase();

  if (/^boolean$/i.test(duckdbType)) {
    return "boolean";
  }

  if (/varchar|char|string|text|uuid/i.test(duckdbType)) {
    return approxCardinality <= 120 ? "categorical" : "text";
  }

  if (isNumericDuckType(duckdbType)) {
    if (
      approxCardinality <= 20 &&
      !/(average|variable|count|score|years?|total|height|weight|ratio|percent)/i.test(lowerName)
    ) {
      return "categorical";
    }

    return "numeric";
  }

  if (/date|time|timestamp/i.test(duckdbType)) {
    return "text";
  }

  return "unknown";
}

function inferTags(name) {
  const tags = new Set();
  const lowerName = name.toLowerCase();

  if (demographicHints.some((hint) => lowerName.includes(hint))) {
    tags.add("demographic");
  }

  if (/(openness|consciensiousness|extroversion|neuroticism|agreeableness)/i.test(name)) {
    tags.add("ocean");
  }

  if (fetishHints.some((hint) => lowerName.includes(hint))) {
    tags.add("fetish");
  }

  if (
    derivedColumns.has(name) ||
    /^total/i.test(name) ||
    /(average|variable)/i.test(name)
  ) {
    tags.add("derived");
  }

  if (tags.size === 0) {
    tags.add("other");
  }

  return [...tags];
}

function normalizeInteger(value, fallback = 0) {
  if (typeof value === "bigint") {
    return Number(value);
  }

  if (typeof value === "number" && Number.isFinite(value)) {
    return value;
  }

  return fallback;
}

function roundRatio(value) {
  if (!Number.isFinite(value)) {
    return 0;
  }

  return Math.round(value * 10000) / 10000;
}

async function main() {
  await stat(sourcePath);

  const instance = await DuckDBInstance.create(":memory:");
  const connection = await instance.connect();

  const sourceLiteral = sourcePath.replaceAll("'", "''");
  await connection.run(`CREATE OR REPLACE VIEW data AS SELECT * FROM read_parquet('${sourceLiteral}')`);

  const rowCountReader = await connection.runAndReadAll("SELECT count(*)::BIGINT AS row_count FROM data");
  const rowCount = normalizeInteger(rowCountReader.getRowsJS()[0]?.[0]);

  const describeReader = await connection.runAndReadAll("DESCRIBE data");
  const describeRows = describeReader.getRowObjectsJS();

  const columns = [];

  for (const row of describeRows) {
    const name = String(row.column_name);
    const duckdbType = String(row.column_type);
    const quotedColumn = quoteIdentifier(name);

    const metricsReader = await connection.runAndReadAll(
      `SELECT
         count(*)::BIGINT AS total_count,
         count(${quotedColumn})::BIGINT AS non_null_count,
         approx_count_distinct(${quotedColumn})::BIGINT AS approx_cardinality
       FROM data`,
    );

    const metrics = metricsReader.getRowObjectsJS()[0] ?? {};
    const totalCount = normalizeInteger(metrics.total_count, rowCount);
    const nonNullCount = normalizeInteger(metrics.non_null_count, 0);
    const approxCardinality = normalizeInteger(metrics.approx_cardinality, 0);

    const nullRatio = totalCount > 0 ? (totalCount - nonNullCount) / totalCount : 0;

    columns.push({
      name,
      duckdbType,
      logicalType: inferLogicalType(name, duckdbType, approxCardinality),
      nullRatio: roundRatio(nullRatio),
      approxCardinality,
      tags: inferTags(name),
    });
  }

  columns.sort((left, right) => left.name.localeCompare(right.name));

  const payload = {
    dataset: {
      name: "Big Kink Survey (Public Sample)",
      sourcePath,
      generatedAt: new Date().toISOString(),
      rowCount,
      columnCount: columns.length,
    },
    columns,
  };

  await mkdir(dirname(outputPath), { recursive: true });
  await writeFile(outputPath, `${JSON.stringify(payload, null, 2)}\n`, "utf8");

  console.log(`Generated ${columns.length} column metadata entries at ${outputPath}`);

  connection.closeSync();
}

main().catch((error) => {
  console.error("Failed to generate column metadata", error);
  process.exit(1);
});
</file>

<file path="scripts/sync-public-data.mjs">
#!/usr/bin/env node

import { copyFile, mkdir, stat } from "node:fs/promises";
import { dirname, resolve } from "node:path";
import { fileURLToPath } from "node:url";

const __dirname = dirname(fileURLToPath(import.meta.url));
const projectRoot = resolve(__dirname, "..");

const sourcePath = process.env.BKS_PARQUET_PATH ?? resolve(projectRoot, "data", "BKSPublic.parquet");
const targetPath = process.env.BKS_PUBLIC_PARQUET_PATH ?? resolve(projectRoot, "public", "BKSPublic.parquet");

async function main() {
  await stat(sourcePath);
  await mkdir(dirname(targetPath), { recursive: true });
  await copyFile(sourcePath, targetPath);
  console.log(`Copied parquet file to ${targetPath}`);
}

main().catch((error) => {
  console.error("Failed to sync parquet into public/", error);
  process.exit(1);
});
</file>

<file path="src/lib/api/contracts.ts">
import { z } from "zod";

export const LogicalTypeSchema = z.enum([
  "categorical",
  "numeric",
  "boolean",
  "text",
  "unknown",
]);

export const CategoryTagSchema = z.enum([
  "demographic",
  "ocean",
  "fetish",
  "derived",
  "other",
]);

export const CaveatKeySchema = z.enum([
  "binned_or_collapsed",
  "combined_or_merged",
  "computed_column",
  "gated_missingness",
  "late_added_questions",
]);

export const CaveatSchema = z.object({
  key: CaveatKeySchema,
  title: z.string(),
  description: z.string(),
  guidance: z.string(),
});

export const ColumnMetadataSchema = z.object({
  name: z.string(),
  duckdbType: z.string(),
  logicalType: LogicalTypeSchema,
  nullRatio: z.number().min(0).max(1),
  approxCardinality: z.number().int().nonnegative(),
  tags: z.array(CategoryTagSchema),
  caveatKeys: z.array(CaveatKeySchema).default([]),
});

export const DatasetMetadataSchema = z.object({
  name: z.string(),
  sourcePath: z.string(),
  generatedAt: z.string(),
  rowCount: z.number().int().nonnegative(),
  columnCount: z.number().int().nonnegative(),
});

export const SchemaDataSchema = z.object({
  dataset: DatasetMetadataSchema,
  columns: z.array(ColumnMetadataSchema),
  caveats: z.object({
    global: z.array(CaveatKeySchema),
    definitions: z.array(CaveatSchema),
  }),
});

export const QueryRequestSchema = z.object({
  sql: z.string().trim().min(1),
  limit: z.number().int().positive().max(10_000).optional(),
});

export const QueryDataSchema = z.object({
  columns: z.array(z.string()),
  rows: z.array(z.array(z.unknown())),
});

export const FilterValueSchema = z.union([z.string(), z.number(), z.boolean(), z.null()]);

export const FiltersSchema = z.record(
  z.string(),
  z.union([FilterValueSchema, z.array(FilterValueSchema)]),
);

export const CrosstabRequestSchema = z.object({
  x: z.string().min(1),
  y: z.string().min(1),
  limit: z.number().int().positive().max(1_000).optional(),
  filters: FiltersSchema.optional(),
});

export const CrosstabRowSchema = z.object({
  x: z.union([z.string(), z.number(), z.boolean(), z.null()]),
  y: z.union([z.string(), z.number(), z.boolean(), z.null()]),
  count: z.number().int().nonnegative(),
});

export const CrosstabDataSchema = z.object({
  x: z.string(),
  y: z.string(),
  rows: z.array(CrosstabRowSchema),
});

export const NumericStatsSchema = z.object({
  kind: z.literal("numeric"),
  totalCount: z.number().int().nonnegative(),
  nonNullCount: z.number().int().nonnegative(),
  nullCount: z.number().int().nonnegative(),
  mean: z.number().nullable(),
  stddev: z.number().nullable(),
  min: z.number().nullable(),
  p25: z.number().nullable(),
  median: z.number().nullable(),
  p75: z.number().nullable(),
  max: z.number().nullable(),
});

export const CategoryCountSchema = z.object({
  value: z.union([z.string(), z.number(), z.boolean(), z.null()]),
  count: z.number().int().nonnegative(),
  percentage: z.number().min(0).max(100),
});

export const CategoricalStatsSchema = z.object({
  kind: z.literal("categorical"),
  totalCount: z.number().int().nonnegative(),
  nonNullCount: z.number().int().nonnegative(),
  nullCount: z.number().int().nonnegative(),
  topValues: z.array(CategoryCountSchema),
});

export const StatsDataSchema = z.object({
  column: z.string(),
  logicalType: LogicalTypeSchema,
  caveatKeys: z.array(CaveatKeySchema),
  stats: z.union([NumericStatsSchema, CategoricalStatsSchema]),
});

export const ApiErrorSchema = z.object({
  code: z.string(),
  message: z.string(),
  details: z.unknown().optional(),
});

export const ApiSuccessEnvelopeSchema = <T extends z.ZodTypeAny>(dataSchema: T) =>
  z.object({
    ok: z.literal(true),
    data: dataSchema,
    meta: z.record(z.string(), z.unknown()).optional(),
  });

export const ApiErrorEnvelopeSchema = z.object({
  ok: z.literal(false),
  error: ApiErrorSchema,
});

export type QueryRequest = z.infer<typeof QueryRequestSchema>;
export type CrosstabRequest = z.infer<typeof CrosstabRequestSchema>;
export type SchemaData = z.infer<typeof SchemaDataSchema>;
export type QueryData = z.infer<typeof QueryDataSchema>;
export type CrosstabData = z.infer<typeof CrosstabDataSchema>;
export type StatsData = z.infer<typeof StatsDataSchema>;
export type ApiError = z.infer<typeof ApiErrorSchema>;
</file>

<file path="src/lib/client/api.ts">
import type { CrosstabData, QueryData, SchemaData, StatsData } from "@/lib/api/contracts";

interface ApiSuccess<T> {
  ok: true;
  data: T;
  meta?: Record<string, unknown>;
}

interface ApiFailure {
  ok: false;
  error: {
    code: string;
    message: string;
    details?: unknown;
  };
}

type ApiEnvelope<T> = ApiSuccess<T> | ApiFailure;

async function fetchEnvelope<T>(url: string, init?: RequestInit): Promise<ApiSuccess<T>> {
  const response = await fetch(url, init);
  const envelope = (await response.json()) as ApiEnvelope<T>;

  if (!response.ok || !envelope.ok) {
    const errorMessage = envelope.ok
      ? `Request failed with status ${response.status}`
      : `${envelope.error.code}: ${envelope.error.message}`;

    throw new Error(errorMessage);
  }

  return envelope;
}

export async function getSchema() {
  return fetchEnvelope<SchemaData>("/api/schema", {
    method: "GET",
  });
}

export async function getStats(column: string) {
  return fetchEnvelope<StatsData>(`/api/stats/${encodeURIComponent(column)}`, {
    method: "GET",
  });
}

export interface CrosstabParams {
  x: string;
  y: string;
  limit?: number;
  filters?: Record<
    string,
    string | number | boolean | null | Array<string | number | boolean | null>
  >;
}

export async function getCrosstab(params: CrosstabParams) {
  const url = new URL("/api/crosstab", window.location.origin);
  url.searchParams.set("x", params.x);
  url.searchParams.set("y", params.y);

  if (params.limit) {
    url.searchParams.set("limit", String(params.limit));
  }

  if (params.filters && Object.keys(params.filters).length > 0) {
    url.searchParams.set("filters", JSON.stringify(params.filters));
  }

  return fetchEnvelope<CrosstabData>(url.toString(), {
    method: "GET",
  });
}

export interface QueryParams {
  sql: string;
  limit?: number;
}

export async function runQuery(params: QueryParams) {
  return fetchEnvelope<QueryData>("/api/query", {
    method: "POST",
    headers: {
      "Content-Type": "application/json",
    },
    body: JSON.stringify(params),
  });
}
</file>

<file path="src/lib/schema/caveats.ts">
export type CaveatKey =
  | "binned_or_collapsed"
  | "combined_or_merged"
  | "computed_column"
  | "gated_missingness"
  | "late_added_questions";

export interface CaveatDefinition {
  key: CaveatKey;
  title: string;
  description: string;
  guidance: string;
}

export const CAVEAT_DEFINITIONS: Record<CaveatKey, CaveatDefinition> = {
  binned_or_collapsed: {
    key: "binned_or_collapsed",
    title: "Binned or Collapsed",
    description:
      "This value was collapsed from a finer-grained original survey scale during data cleaning.",
    guidance:
      "Interpret as broad buckets, not the full original response distribution.",
  },
  combined_or_merged: {
    key: "combined_or_merged",
    title: "Combined or Merged",
    description:
      "This column combines multiple source questions or categories into one derived field.",
    guidance:
      "Use caution when comparing to the original GT survey wording.",
  },
  computed_column: {
    key: "computed_column",
    title: "Computed",
    description:
      "This value is algorithmically computed (average/total/derived score), not a direct survey answer.",
    guidance:
      "Treat as a modelled score rather than a literal response option.",
  },
  gated_missingness: {
    key: "gated_missingness",
    title: "Gated Missingness",
    description:
      "Many questions are shown only after prerequisite answers. Missing values may be structural, not random.",
    guidance:
      "When domain-appropriate, consider NA as implicit zero or not-applicable rather than unknown.",
  },
  late_added_questions: {
    key: "late_added_questions",
    title: "Late-Added Questions",
    description:
      "Some questions were added later in survey history, which can create cohort-related missingness.",
    guidance:
      "Compare missingness rates before assuming behavioral differences.",
  },
};

export const GLOBAL_CAVEAT_KEYS: CaveatKey[] = ["gated_missingness", "late_added_questions"];

const COLUMN_CAVEAT_PATTERNS: Array<{ pattern: RegExp; keys: CaveatKey[] }> = [
  { pattern: /^politics$/i, keys: ["binned_or_collapsed"] },
  { pattern: /^bmi$/i, keys: ["binned_or_collapsed"] },
  { pattern: /^straightness$/i, keys: ["binned_or_collapsed", "computed_column"] },
  { pattern: /^sexcount$/i, keys: ["binned_or_collapsed"] },
  {
    pattern: /How "sexually liberated" was your upbringing\? \(fs700v2\)/i,
    keys: ["binned_or_collapsed"],
  },
  { pattern: /^childhood_adversity$/i, keys: ["combined_or_merged"] },
  { pattern: /^childhood_gender_tolerance$/i, keys: ["combined_or_merged"] },
  { pattern: /^TotalMentalIllness$/i, keys: ["combined_or_merged"] },
  {
    pattern: /preferred relationship style.*\(4jib23m\)/i,
    keys: ["combined_or_merged"],
  },
  {
    pattern:
      /(opennessvariable|consciensiousnessvariable|extroversionvariable|neuroticismvariable|agreeablenessvariable|powerlessnessvariable|totalfetishcategory|bondageaverage)/i,
    keys: ["computed_column"],
  },
  {
    pattern: /^Total[A-Za-z0-9_]+/,
    keys: ["computed_column"],
  },
];

function dedupe(keys: CaveatKey[]): CaveatKey[] {
  return [...new Set(keys)];
}

export function getCaveatKeysForColumn(columnName: string): CaveatKey[] {
  const specific = COLUMN_CAVEAT_PATTERNS.flatMap((entry) =>
    entry.pattern.test(columnName) ? entry.keys : [],
  );

  return dedupe([...specific, ...GLOBAL_CAVEAT_KEYS]);
}

export function getCaveatsForColumn(columnName: string): CaveatDefinition[] {
  return getCaveatKeysForColumn(columnName).map((key) => CAVEAT_DEFINITIONS[key]);
}

export function getGlobalCaveats(): CaveatDefinition[] {
  return GLOBAL_CAVEAT_KEYS.map((key) => CAVEAT_DEFINITIONS[key]);
}

export function formatCaveatHint(columnName: string): string {
  const keys = getCaveatKeysForColumn(columnName);
  if (keys.length === 0) {
    return "No caveats.";
  }

  return keys.map((key) => CAVEAT_DEFINITIONS[key].title).join(", ");
}
</file>

<file path="src/lib/schema/metadata.ts">
import rawSchemaMetadata from "./columns.generated.json";
import { getCaveatKeysForColumn } from "./caveats";
import type { ColumnMetadata, SchemaMetadata } from "./types";

const schemaMetadata = rawSchemaMetadata as SchemaMetadata;

const columnByName = new Map<string, ColumnMetadata>(
  schemaMetadata.columns.map((column) => [column.name, column]),
);

export function getSchemaMetadata(): SchemaMetadata {
  return schemaMetadata;
}

export function getColumnMetadata(columnName: string): ColumnMetadata | undefined {
  return columnByName.get(columnName);
}

export function listColumns(): ColumnMetadata[] {
  return schemaMetadata.columns;
}

export function listColumnsWithCaveats() {
  return schemaMetadata.columns.map((column) => ({
    ...column,
    caveatKeys: getCaveatKeysForColumn(column.name),
  }));
}
</file>

<file path="src/lib/schema/types.ts">
export type LogicalType = "categorical" | "numeric" | "boolean" | "text" | "unknown";

export type CategoryTag = "demographic" | "ocean" | "fetish" | "derived" | "other";

export interface ColumnMetadata {
  name: string;
  duckdbType: string;
  logicalType: LogicalType;
  nullRatio: number;
  approxCardinality: number;
  tags: CategoryTag[];
}

export interface DatasetMetadata {
  name: string;
  sourcePath: string;
  generatedAt: string;
  rowCount: number;
  columnCount: number;
}

export interface SchemaMetadata {
  dataset: DatasetMetadata;
  columns: ColumnMetadata[];
}
</file>

<file path="src/lib/server/api-response.ts">
import type { ApiError } from "@/lib/api/contracts";

const defaultHeaders = {
  "Content-Type": "application/json",
};

export function jsonResponse(payload: unknown, status = 200, headers: HeadersInit = {}) {
  return new Response(JSON.stringify(payload), {
    status,
    headers: {
      ...defaultHeaders,
      ...headers,
    },
  });
}

export function okResponse(data: unknown, meta?: Record<string, unknown>, status = 200) {
  return jsonResponse(
    {
      ok: true,
      data,
      ...(meta ? { meta } : {}),
    },
    status,
  );
}

export function errorResponse(status: number, error: ApiError) {
  return jsonResponse(
    {
      ok: false,
      error,
    },
    status,
  );
}
</file>

<file path="src/lib/server/db.ts">
import { execFile } from "node:child_process";
import { existsSync } from "node:fs";
import { resolve } from "node:path";
import { promisify } from "node:util";

import { DEFAULT_QUERY_TIMEOUT_MS } from "./sql-guards";

const execFileAsync = promisify(execFile);
const maxBufferBytes = 24 * 1024 * 1024;
let nodeApiInstancePromise: Promise<any> | null = null;

export const DATA_TABLE_NAME = "data";

export class QueryExecutionError extends Error {
  readonly code: string;

  constructor(code: string, message: string) {
    super(message);
    this.code = code;
    this.name = "QueryExecutionError";
  }
}

export interface QueryResult {
  columns: string[];
  rows: unknown[][];
  rowObjects: Array<Record<string, unknown>>;
}

function parquetPath(): string {
  return process.env.BKS_PARQUET_PATH ?? resolve(process.cwd(), "data", "BKSPublic.parquet");
}

function parquetLiteral(): string {
  return parquetPath().replaceAll("'", "''");
}

function ensureParquetExists() {
  const sourcePath = parquetPath();
  if (!existsSync(sourcePath)) {
    throw new QueryExecutionError("PARQUET_NOT_FOUND", `Parquet file not found: ${sourcePath}`);
  }
}

function prepareSql(userSql: string): string {
  return `
    CREATE OR REPLACE TEMP VIEW ${DATA_TABLE_NAME} AS
    SELECT * FROM read_parquet('${parquetLiteral()}');

    ${userSql}
  `;
}

function normalizeValue(value: unknown): unknown {
  if (value == null) {
    return null;
  }

  if (typeof value === "bigint") {
    const numeric = Number(value);
    return Number.isSafeInteger(numeric) ? numeric : value.toString();
  }

  if (typeof value === "string") {
    const trimmed = value.trim();
    if (trimmed === "Infinity" || trimmed === "-Infinity" || trimmed === "NaN") {
      return null;
    }
  }

  return value;
}

function normalizeRow(row: Record<string, unknown>): Record<string, unknown> {
  return Object.fromEntries(
    Object.entries(row).map(([key, value]) => [key, normalizeValue(value)]),
  );
}

function isMissingExecutable(error: unknown): boolean {
  return (
    typeof error === "object" &&
    error !== null &&
    "code" in error &&
    (error as { code?: string }).code === "ENOENT"
  );
}

async function getNodeApiInstance() {
  if (!nodeApiInstancePromise) {
    const moduleName = "@duckdb/node-api";
    nodeApiInstancePromise = import(/* @vite-ignore */ moduleName).then((module) =>
      module.DuckDBInstance.create(":memory:"),
    );
  }

  return nodeApiInstancePromise;
}

async function executeDuckDbNodeApi(
  sql: string,
  timeoutMs: number,
): Promise<Array<Record<string, unknown>>> {
  ensureParquetExists();

  const instance = await getNodeApiInstance();
  const connection = await instance.connect();
  let timer: ReturnType<typeof setTimeout> | undefined;

  try {
    await connection.run(
      `CREATE OR REPLACE TEMP VIEW ${DATA_TABLE_NAME} AS SELECT * FROM read_parquet('${parquetLiteral()}')`,
    );

    const queryPromise = connection.runAndReadAll(sql).then((reader: any) =>
      reader.getRowObjectsJS() as Array<Record<string, unknown>>,
    );

    if (timeoutMs > 0) {
      const timeoutPromise = new Promise<never>((_resolve, reject) => {
        timer = setTimeout(() => {
          void connection.interrupt().catch(() => {
            // no-op
          });
          reject(new QueryExecutionError("QUERY_TIMEOUT", `Query exceeded ${timeoutMs}ms timeout.`));
        }, timeoutMs);
      });

      const rows = (await Promise.race([queryPromise, timeoutPromise])) as Array<
        Record<string, unknown>
      >;
      return rows.map((row) => normalizeRow(row));
    }

    const rows: Array<Record<string, unknown>> = await queryPromise;
    return rows.map((row) => normalizeRow(row));
  } catch (error: unknown) {
    if (error instanceof QueryExecutionError) {
      throw error;
    }

    throw new QueryExecutionError(
      "QUERY_EXECUTION_FAILED",
      error instanceof Error ? error.message : "DuckDB query failed.",
    );
  } finally {
    if (timer) {
      clearTimeout(timer);
    }
    connection.closeSync();
  }
}

async function executeDuckDbJson(sql: string, timeoutMs: number): Promise<Array<Record<string, unknown>>> {
  ensureParquetExists();

  try {
    const { stdout } = await execFileAsync(
      "duckdb",
      ["-json", ":memory:", "-c", sql],
      {
        timeout: timeoutMs,
        maxBuffer: maxBufferBytes,
      },
    );

    const trimmed = stdout.trim();
    if (!trimmed) {
      return [];
    }

    const parsed = JSON.parse(trimmed) as unknown;
    if (!Array.isArray(parsed)) {
      throw new QueryExecutionError("INVALID_DUCKDB_OUTPUT", "DuckDB did not return a JSON array.");
    }

    return parsed.map((row) => normalizeRow((row ?? {}) as Record<string, unknown>));
  } catch (error: unknown) {
    if (isMissingExecutable(error)) {
      return executeDuckDbNodeApi(sql, timeoutMs);
    }

    if (error && typeof error === "object" && "code" in error && error.code === "ETIMEDOUT") {
      throw new QueryExecutionError("QUERY_TIMEOUT", `Query exceeded ${timeoutMs}ms timeout.`);
    }

    const message =
      error && typeof error === "object" && "stderr" in error && typeof error.stderr === "string"
        ? error.stderr.trim() || "DuckDB query failed."
        : error instanceof Error
          ? error.message
          : "DuckDB query failed.";

    throw new QueryExecutionError("QUERY_EXECUTION_FAILED", message);
  }
}

export async function runQuery(
  userSql: string,
  timeoutMs = DEFAULT_QUERY_TIMEOUT_MS,
): Promise<QueryResult> {
  const rowObjects = await executeDuckDbJson(prepareSql(userSql), timeoutMs);
  const columns = rowObjects.length > 0 ? Object.keys(rowObjects[0]) : [];
  const rows = rowObjects.map((row) => columns.map((column) => row[column] ?? null));

  return {
    columns,
    rows,
    rowObjects,
  };
}

export async function runSingleRow(
  userSql: string,
  timeoutMs = DEFAULT_QUERY_TIMEOUT_MS,
): Promise<Record<string, unknown>> {
  const result = await runQuery(userSql, timeoutMs);
  return result.rowObjects[0] ?? {};
}
</file>

<file path="src/lib/server/sql-guards.ts">
export const DEFAULT_QUERY_LIMIT = 1_000;
export const HARD_QUERY_LIMIT = 10_000;
export const DEFAULT_QUERY_TIMEOUT_MS = 5_000;

const allowedLeadingKeywords = /^(SELECT|WITH|DESCRIBE|EXPLAIN)\b/i;

const blockedTokens = [
  "INSERT",
  "UPDATE",
  "DELETE",
  "DROP",
  "ALTER",
  "CREATE",
  "COPY",
  "ATTACH",
  "DETACH",
  "INSTALL",
  "LOAD",
  "PRAGMA",
  "CALL",
  "EXPORT",
  "IMPORT",
  "VACUUM",
  "TRUNCATE",
  "MERGE",
  "REPLACE",
  "GRANT",
  "REVOKE",
];

const blockedRegex = new RegExp(`\\b(${blockedTokens.join("|")})\\b`, "i");

export class SqlGuardError extends Error {
  readonly code: string;

  constructor(code: string, message: string) {
    super(message);
    this.code = code;
    this.name = "SqlGuardError";
  }
}

export function normalizeSql(rawSql: string): string {
  return rawSql.trim().replace(/;+$/g, "");
}

export function ensureReadOnlySql(rawSql: string): string {
  const normalizedSql = normalizeSql(rawSql);

  if (normalizedSql.length === 0) {
    throw new SqlGuardError("EMPTY_SQL", "SQL must not be empty.");
  }

  if (normalizedSql.includes(";")) {
    throw new SqlGuardError(
      "MULTI_STATEMENT_BLOCKED",
      "Only a single read-only SQL statement is allowed.",
    );
  }

  if (!allowedLeadingKeywords.test(normalizedSql)) {
    throw new SqlGuardError(
      "READ_ONLY_REQUIRED",
      "Only SELECT/WITH/DESCRIBE/EXPLAIN statements are allowed.",
    );
  }

  if (blockedRegex.test(normalizedSql)) {
    throw new SqlGuardError(
      "MUTATING_SQL_BLOCKED",
      "Mutating SQL keywords are not allowed.",
    );
  }

  return normalizedSql;
}

export function clampLimit(limit: number | undefined, defaultLimit = DEFAULT_QUERY_LIMIT): number {
  if (limit == null || Number.isNaN(limit)) {
    return defaultLimit;
  }

  if (limit < 1) {
    return 1;
  }

  if (limit > HARD_QUERY_LIMIT) {
    return HARD_QUERY_LIMIT;
  }

  return limit;
}

export function applyLimitToQuery(sql: string, limit: number): string {
  const normalized = normalizeSql(sql);

  if (/^(DESCRIBE|EXPLAIN)\b/i.test(normalized)) {
    return normalized;
  }

  return `SELECT * FROM (${normalized}) AS bounded_query LIMIT ${limit}`;
}

export function quoteIdentifier(identifier: string): string {
  return `"${identifier.replaceAll('"', '""')}"`;
}

export function quoteLiteral(value: string | number | boolean | null): string {
  if (value === null) {
    return "NULL";
  }

  if (typeof value === "number") {
    if (!Number.isFinite(value)) {
      throw new SqlGuardError("INVALID_LITERAL", "Non-finite numeric values are not supported.");
    }

    return String(value);
  }

  if (typeof value === "boolean") {
    return value ? "TRUE" : "FALSE";
  }

  return `'${value.replaceAll("'", "''")}'`;
}

export type FilterValue = string | number | boolean | null;
export type FilterInput = FilterValue | FilterValue[];

export function buildWhereClause(filters: Record<string, FilterInput> | undefined): string {
  if (!filters || Object.keys(filters).length === 0) {
    return "";
  }

  const predicates: string[] = [];

  for (const [columnName, rawValue] of Object.entries(filters)) {
    const columnIdentifier = quoteIdentifier(columnName);

    if (Array.isArray(rawValue)) {
      const nonNullValues = rawValue.filter((value) => value !== null);
      const hasNull = rawValue.length !== nonNullValues.length;

      const inPredicate =
        nonNullValues.length > 0
          ? `${columnIdentifier} IN (${nonNullValues.map((value) => quoteLiteral(value)).join(", ")})`
          : "";

      if (hasNull && inPredicate) {
        predicates.push(`(${inPredicate} OR ${columnIdentifier} IS NULL)`);
      } else if (hasNull) {
        predicates.push(`${columnIdentifier} IS NULL`);
      } else if (inPredicate) {
        predicates.push(inPredicate);
      }

      continue;
    }

    if (rawValue === null) {
      predicates.push(`${columnIdentifier} IS NULL`);
      continue;
    }

    predicates.push(`${columnIdentifier} = ${quoteLiteral(rawValue)}`);
  }

  if (predicates.length === 0) {
    return "";
  }

  return `WHERE ${predicates.join(" AND ")}`;
}
</file>

<file path="src/routes/api/crosstab.ts">
import { createFileRoute } from "@tanstack/react-router";
import { CrosstabDataSchema, CrosstabRequestSchema } from "@/lib/api/contracts";
import { errorResponse, okResponse } from "@/lib/server/api-response";
import { QueryExecutionError, runQuery } from "@/lib/server/db";
import {
  buildWhereClause,
  clampLimit,
  quoteIdentifier,
} from "@/lib/server/sql-guards";
import { getColumnMetadata } from "@/lib/schema/metadata";

export const Route = createFileRoute("/api/crosstab")({
  server: {
    handlers: {
      GET: async ({ request }) => {
        const url = new URL(request.url);

        const rawLimit = url.searchParams.get("limit");
        const rawFilters = url.searchParams.get("filters");

        let parsedFilters: unknown;
        if (rawFilters) {
          try {
            parsedFilters = JSON.parse(rawFilters);
          } catch {
            return errorResponse(400, {
              code: "INVALID_FILTERS",
              message: "filters must be valid JSON.",
            });
          }
        }

        const requestParams = CrosstabRequestSchema.safeParse({
          x: url.searchParams.get("x") ?? "",
          y: url.searchParams.get("y") ?? "",
          limit: rawLimit ? Number(rawLimit) : undefined,
          filters: parsedFilters,
        });

        if (!requestParams.success) {
          return errorResponse(400, {
            code: "INVALID_REQUEST",
            message: "Query parameters failed validation.",
            details: requestParams.error.flatten(),
          });
        }

        const { x, y, filters } = requestParams.data;
        const limit = clampLimit(requestParams.data.limit, 200);

        const xMeta = getColumnMetadata(x);
        const yMeta = getColumnMetadata(y);

        if (!xMeta || !yMeta) {
          return errorResponse(404, {
            code: "COLUMN_NOT_FOUND",
            message: "x or y column was not found in schema metadata.",
          });
        }

        const whereClause = buildWhereClause(filters);
        const sql = `
          SELECT
            ${quoteIdentifier(x)} AS x,
            ${quoteIdentifier(y)} AS y,
            count(*)::BIGINT AS count
          FROM data
          ${whereClause}
          GROUP BY 1, 2
          ORDER BY count DESC
          LIMIT ${limit}
        `;

        try {
          const result = await runQuery(sql);
          const rows = result.rows.map((row) => ({
            x: (row[0] ?? null) as string | number | boolean | null,
            y: (row[1] ?? null) as string | number | boolean | null,
            count: Number(row[2] ?? 0),
          }));

          const data = CrosstabDataSchema.parse({
            x,
            y,
            rows,
          });

          return okResponse(data, {
            limit,
            rowCount: rows.length,
          });
        } catch (error) {
          if (error instanceof QueryExecutionError) {
            return errorResponse(error.code === "QUERY_TIMEOUT" ? 408 : 400, {
              code: error.code,
              message: error.message,
            });
          }

          return errorResponse(500, {
            code: "CROSSTAB_FAILED",
            message: "Failed to execute crosstab query.",
          });
        }
      },
    },
  },
});
</file>

<file path="src/routes/api/health.ts">
import { createFileRoute } from "@tanstack/react-router";
import { okResponse } from "@/lib/server/api-response";

export const Route = createFileRoute("/api/health")({
  server: {
    handlers: {
      GET: async () => {
        return okResponse(
          {
            status: "healthy",
            timestamp: new Date().toISOString(),
          },
          undefined,
          200,
        );
      },
    },
  },
});
</file>

<file path="src/routes/api/query.ts">
import { createFileRoute } from "@tanstack/react-router";
import { QueryRequestSchema } from "@/lib/api/contracts";
import { errorResponse, okResponse } from "@/lib/server/api-response";
import { QueryExecutionError, runQuery } from "@/lib/server/db";
import {
  applyLimitToQuery,
  clampLimit,
  ensureReadOnlySql,
  SqlGuardError,
} from "@/lib/server/sql-guards";

export const Route = createFileRoute("/api/query")({
  server: {
    handlers: {
      POST: async ({ request }) => {
        let rawBody: unknown;
        try {
          rawBody = await request.json();
        } catch {
          return errorResponse(400, {
            code: "INVALID_JSON",
            message: "Request body must be valid JSON.",
          });
        }

        const parsedBody = QueryRequestSchema.safeParse(rawBody);
        if (!parsedBody.success) {
          return errorResponse(400, {
            code: "INVALID_REQUEST",
            message: "Request payload failed validation.",
            details: parsedBody.error.flatten(),
          });
        }

        const requestData = parsedBody.data;
        const limit = clampLimit(requestData.limit);

        let readOnlySql: string;
        try {
          readOnlySql = ensureReadOnlySql(requestData.sql);
        } catch (error) {
          if (error instanceof SqlGuardError) {
            return errorResponse(400, {
              code: error.code,
              message: error.message,
            });
          }

          return errorResponse(400, {
            code: "INVALID_SQL",
            message: "SQL validation failed.",
          });
        }

        const boundedSql = applyLimitToQuery(readOnlySql, limit);

        try {
          const result = await runQuery(boundedSql);

          return okResponse(
            {
              columns: result.columns,
              rows: result.rows,
            },
            {
              limit,
              rowCount: result.rows.length,
              queryKind: readOnlySql.split(/\s+/)[0]?.toUpperCase() ?? "UNKNOWN",
            },
          );
        } catch (error) {
          if (error instanceof QueryExecutionError) {
            const status = error.code === "QUERY_TIMEOUT" ? 408 : 400;
            return errorResponse(status, {
              code: error.code,
              message: error.message,
            });
          }

          return errorResponse(500, {
            code: "QUERY_FAILED",
            message: "Query execution failed.",
          });
        }
      },
    },
  },
});
</file>

<file path="src/routes/api/schema.ts">
import { createFileRoute } from "@tanstack/react-router";
import { SchemaDataSchema } from "@/lib/api/contracts";
import { okResponse } from "@/lib/server/api-response";
import {
  CAVEAT_DEFINITIONS,
  GLOBAL_CAVEAT_KEYS,
} from "@/lib/schema/caveats";
import { getSchemaMetadata, listColumnsWithCaveats } from "@/lib/schema/metadata";

export const Route = createFileRoute("/api/schema")({
  server: {
    handlers: {
      GET: async () => {
        const schemaMetadata = getSchemaMetadata();
        const payload = {
          dataset: schemaMetadata.dataset,
          columns: listColumnsWithCaveats(),
          caveats: {
            global: GLOBAL_CAVEAT_KEYS,
            definitions: Object.values(CAVEAT_DEFINITIONS),
          },
        };

        const data = SchemaDataSchema.parse(payload);

        return okResponse(data, {
          cacheTtlSeconds: 3_600,
        });
      },
    },
  },
});
</file>

<file path="src/routes/api/stats.$column.ts">
import { createFileRoute } from "@tanstack/react-router";
import { StatsDataSchema } from "@/lib/api/contracts";
import { errorResponse, okResponse } from "@/lib/server/api-response";
import { QueryExecutionError, runQuery, runSingleRow } from "@/lib/server/db";
import { quoteIdentifier } from "@/lib/server/sql-guards";
import { getCaveatKeysForColumn } from "@/lib/schema/caveats";
import { getColumnMetadata } from "@/lib/schema/metadata";

function asNumber(value: unknown, fallback = 0): number {
  if (typeof value === "number") {
    return Number.isFinite(value) ? value : fallback;
  }

  if (typeof value === "string") {
    const numeric = Number(value);
    return Number.isFinite(numeric) ? numeric : fallback;
  }

  return fallback;
}

function asNullableNumber(value: unknown): number | null {
  if (value == null) {
    return null;
  }

  const numeric = asNumber(value, Number.NaN);
  return Number.isNaN(numeric) ? null : numeric;
}

export const Route = createFileRoute("/api/stats/$column")({
  server: {
    handlers: {
      GET: async ({ params }) => {
        const column = decodeURIComponent(params.column);
        const metadata = getColumnMetadata(column);

        if (!metadata) {
          return errorResponse(404, {
            code: "COLUMN_NOT_FOUND",
            message: `Column '${column}' not found.`,
          });
        }

        const quotedColumn = quoteIdentifier(column);

        try {
          const counts = await runSingleRow(
            `SELECT
                 count(*)::BIGINT AS total_count,
                 count(${quotedColumn})::BIGINT AS non_null_count,
                 (count(*) - count(${quotedColumn}))::BIGINT AS null_count
               FROM data`,
          );

          const totalCount = asNumber(counts.total_count, 0);
          const nonNullCount = asNumber(counts.non_null_count, 0);
          const nullCount = asNumber(counts.null_count, 0);

          if (metadata.logicalType === "numeric") {
            const numericSummary = await runSingleRow(
              `SELECT
                   avg(${quotedColumn})::DOUBLE AS mean,
                   stddev_samp(${quotedColumn})::DOUBLE AS stddev,
                   min(${quotedColumn})::DOUBLE AS min,
                   quantile_cont(${quotedColumn}, 0.25)::DOUBLE AS p25,
                   median(${quotedColumn})::DOUBLE AS median,
                   quantile_cont(${quotedColumn}, 0.75)::DOUBLE AS p75,
                   max(${quotedColumn})::DOUBLE AS max
                 FROM data
                 WHERE ${quotedColumn} IS NOT NULL`,
            );

            const data = StatsDataSchema.parse({
              column,
              logicalType: metadata.logicalType,
              caveatKeys: getCaveatKeysForColumn(column),
              stats: {
                kind: "numeric",
                totalCount,
                nonNullCount,
                nullCount,
                mean: asNullableNumber(numericSummary.mean),
                stddev: asNullableNumber(numericSummary.stddev),
                min: asNullableNumber(numericSummary.min),
                p25: asNullableNumber(numericSummary.p25),
                median: asNullableNumber(numericSummary.median),
                p75: asNullableNumber(numericSummary.p75),
                max: asNullableNumber(numericSummary.max),
              },
            });

            return okResponse(data);
          }

          const topValuesResult = await runQuery(
            `SELECT
                 cast(${quotedColumn} AS VARCHAR) AS value,
                 count(*)::BIGINT AS count
               FROM data
               WHERE ${quotedColumn} IS NOT NULL
               GROUP BY 1
               ORDER BY count DESC
               LIMIT 12`,
          );

          const topValues = topValuesResult.rows.map((row) => {
            const count = asNumber(row[1], 0);
            return {
              value: (row[0] ?? null) as string | number | boolean | null,
              count,
              percentage:
                nonNullCount > 0
                  ? Math.round((count / nonNullCount) * 10_000) / 100
                  : 0,
            };
          });

          const data = StatsDataSchema.parse({
            column,
            logicalType: metadata.logicalType,
            caveatKeys: getCaveatKeysForColumn(column),
            stats: {
              kind: "categorical",
              totalCount,
              nonNullCount,
              nullCount,
              topValues,
            },
          });

          return okResponse(data);
        } catch (error) {
          if (error instanceof QueryExecutionError) {
            return errorResponse(error.code === "QUERY_TIMEOUT" ? 408 : 400, {
              code: error.code,
              message: error.message,
            });
          }

          return errorResponse(500, {
            code: "STATS_FAILED",
            message: "Failed to compute column statistics.",
          });
        }
      },
    },
  },
});
</file>

<file path="src/routes/__root.tsx">
import {
  HeadContent,
  Link,
  Outlet,
  Scripts,
  createRootRoute,
} from "@tanstack/react-router";

import { DuckDBProvider } from "@/lib/duckdb/provider";
import appCss from "../styles.css?url";

export const Route = createRootRoute({
  head: () => ({
    meta: [
      { charSet: "utf-8" },
      { name: "viewport", content: "width=device-width, initial-scale=1" },
      { title: "Big Kink Survey Explorer" },
    ],
    links: [{ rel: "stylesheet", href: appCss }],
  }),
  component: RootComponent,
  shellComponent: RootDocument,
});

function RootDocument({ children }: { children: React.ReactNode }) {
  return (
    <html lang="en">
      <head>
        <HeadContent />
      </head>
      <body>
        {children}
        <Scripts />
      </body>
    </html>
  );
}

const navLinks = [
  { to: "/", label: "Dashboard" },
  { to: "/explore", label: "Explore" },
  { to: "/profile", label: "Profile" },
  { to: "/sql", label: "SQL" },
] as const;

function RootComponent() {
  return (
    <DuckDBProvider>
      <div className="min-h-screen bg-slate-950 text-slate-100">
        <nav className="border-b border-slate-800 px-6 py-3">
          <div className="mx-auto flex max-w-7xl items-center gap-6">
            <span className="text-lg font-bold">BKS Explorer</span>
            <div className="flex gap-4">
              {navLinks.map((link) => (
                <Link
                  key={link.to}
                  to={link.to}
                  className="text-sm text-slate-400 hover:text-white transition-colors [&.active]:text-white [&.active]:font-medium"
                >
                  {link.label}
                </Link>
              ))}
            </div>
          </div>
        </nav>
        <main className="mx-auto max-w-7xl px-6 py-8">
          <Outlet />
        </main>
      </div>
    </DuckDBProvider>
  );
}
</file>

<file path="src/routes/explore.tsx">
import { createFileRoute } from "@tanstack/react-router";
import { useEffect, useMemo, useState } from "react";

import type { SchemaData } from "@/lib/api/contracts";
import { getSchema } from "@/lib/client/api";
import { useDuckDBQuery } from "@/lib/duckdb/use-query";
import { buildWhereClause, quoteIdentifier } from "@/lib/duckdb/sql-helpers";
import type { FilterInput } from "@/lib/duckdb/sql-helpers";

export const Route = createFileRoute("/explore")({
  component: ExplorePage,
});

function asNumber(value: unknown, fallback = 0): number {
  if (typeof value === "number") return Number.isFinite(value) ? value : fallback;
  if (typeof value === "string") {
    const n = Number(value);
    return Number.isFinite(n) ? n : fallback;
  }
  return fallback;
}

function ExplorePage() {
  const [schema, setSchema] = useState<SchemaData | null>(null);
  const [schemaError, setSchemaError] = useState<string | null>(null);

  const [xColumn, setXColumn] = useState("");
  const [yColumn, setYColumn] = useState("");
  const [limit, setLimit] = useState(50);

  const [filterColumn, setFilterColumn] = useState("");
  const [selectedFilterValues, setSelectedFilterValues] = useState<string[]>([]);

  useEffect(() => {
    let cancelled = false;

    void getSchema()
      .then((response) => {
        if (cancelled) return;
        setSchema(response.data);
        const preferredX = response.data.columns.find((c) => c.name === "straightness");
        const preferredY = response.data.columns.find((c) => c.name === "politics");
        setXColumn(preferredX?.name ?? response.data.columns[0]?.name ?? "");
        setYColumn(preferredY?.name ?? response.data.columns[1]?.name ?? "");

        const firstDemoFilter = response.data.columns.find(
          (c) => c.tags.includes("demographic") && c.logicalType === "categorical",
        );
        setFilterColumn(firstDemoFilter?.name ?? "");
      })
      .catch((error: Error) => {
        if (!cancelled) setSchemaError(error.message);
      });

    return () => { cancelled = true; };
  }, []);

  // Get filter options via DuckDB-WASM
  const filterOptionsSql = useMemo(() => {
    if (!filterColumn) return null;
    const quoted = quoteIdentifier(filterColumn);
    return `SELECT cast(${quoted} AS VARCHAR) AS value, count(*)::BIGINT AS cnt FROM data WHERE ${quoted} IS NOT NULL GROUP BY 1 ORDER BY cnt DESC LIMIT 12`;
  }, [filterColumn]);

  const filterOptionsQuery = useDuckDBQuery(filterOptionsSql);

  const filterOptions = useMemo(() => {
    if (!filterOptionsQuery.data) return [];
    return filterOptionsQuery.data.rows.map((r) => ({
      value: String(r[0] ?? "NULL"),
      count: asNumber(r[1]),
    }));
  }, [filterOptionsQuery.data]);

  // Reset selected filter values when filter column changes
  useEffect(() => {
    setSelectedFilterValues([]);
  }, [filterColumn]);

  const queryFilters = useMemo(() => {
    if (!filterColumn || selectedFilterValues.length === 0) return undefined;
    return {
      [filterColumn]: selectedFilterValues.map((v) => (v === "NULL" ? null : v)),
    } as Record<string, FilterInput>;
  }, [filterColumn, selectedFilterValues]);

  // Crosstab query via DuckDB-WASM
  const crosstabSql = useMemo(() => {
    if (!xColumn || !yColumn) return null;
    const whereClause = buildWhereClause(queryFilters);
    return `SELECT ${quoteIdentifier(xColumn)} AS x, ${quoteIdentifier(yColumn)} AS y, count(*)::BIGINT AS count FROM data ${whereClause} GROUP BY 1, 2 ORDER BY count DESC LIMIT ${limit}`;
  }, [xColumn, yColumn, limit, queryFilters]);

  const crosstabQuery = useDuckDBQuery(crosstabSql);

  const result = useMemo(() => {
    if (!crosstabQuery.data || !xColumn || !yColumn) return null;
    return {
      x: xColumn,
      y: yColumn,
      rows: crosstabQuery.data.rows.map((r) => ({
        x: (r[0] ?? null) as string | number | boolean | null,
        y: (r[1] ?? null) as string | number | boolean | null,
        count: asNumber(r[2]),
      })),
    };
  }, [crosstabQuery.data, xColumn, yColumn]);

  const demographicColumns = useMemo(() => {
    if (!schema) return [];
    return schema.columns.filter(
      (c) => c.tags.includes("demographic") && c.logicalType === "categorical",
    );
  }, [schema]);

  return (
    <div className="space-y-6">
      <header className="space-y-2">
        <h1 className="text-3xl font-bold tracking-tight">Cross-Tab Explorer</h1>
        <p className="text-slate-300">Choose two variables and optional demographic filters.</p>
      </header>

      {schemaError ? (
        <section className="rounded-lg border border-red-500/40 bg-red-900/20 p-4 text-red-200">
          Failed to load schema: {schemaError}
        </section>
      ) : null}

      {schema ? (
        <section className="grid gap-4 rounded-lg border border-slate-800 bg-slate-900/70 p-4 lg:grid-cols-2">
          <label className="text-sm text-slate-300">
            X column
            <select
              className="mt-1 w-full rounded border border-slate-700 bg-slate-950 px-2 py-2"
              value={xColumn}
              onChange={(event) => setXColumn(event.target.value)}
            >
              {schema.columns.map((column) => (
                <option key={column.name} value={column.name}>
                  {column.name}
                </option>
              ))}
            </select>
          </label>

          <label className="text-sm text-slate-300">
            Y column
            <select
              className="mt-1 w-full rounded border border-slate-700 bg-slate-950 px-2 py-2"
              value={yColumn}
              onChange={(event) => setYColumn(event.target.value)}
            >
              {schema.columns.map((column) => (
                <option key={column.name} value={column.name}>
                  {column.name}
                </option>
              ))}
            </select>
          </label>

          <label className="text-sm text-slate-300">
            Result row limit
            <input
              type="number"
              min={1}
              max={1000}
              className="mt-1 w-full rounded border border-slate-700 bg-slate-950 px-2 py-2"
              value={limit}
              onChange={(event) => setLimit(Math.max(1, Math.min(1000, Number(event.target.value) || 1)))}
            />
          </label>

          <label className="text-sm text-slate-300">
            Optional demographic filter
            <select
              className="mt-1 w-full rounded border border-slate-700 bg-slate-950 px-2 py-2"
              value={filterColumn}
              onChange={(event) => setFilterColumn(event.target.value)}
            >
              <option value="">None</option>
              {demographicColumns.map((column) => (
                <option key={column.name} value={column.name}>
                  {column.name}
                </option>
              ))}
            </select>
          </label>

          {filterColumn && filterOptions.length > 0 ? (
            <div className="lg:col-span-2">
              <p className="mb-2 text-sm text-slate-300">Filter values</p>
              <div className="grid gap-2 sm:grid-cols-2 lg:grid-cols-3">
                {filterOptions.map((option) => {
                  const checked = selectedFilterValues.includes(option.value);
                  return (
                    <label
                      key={option.value}
                      className="flex items-center justify-between rounded border border-slate-700 bg-slate-950 px-3 py-2 text-sm text-slate-200"
                    >
                      <span className="truncate pr-2">{option.value}</span>
                      <span className="ml-2 shrink-0 text-xs text-slate-400">{option.count}</span>
                      <input
                        className="ml-3"
                        type="checkbox"
                        checked={checked}
                        onChange={(event) => {
                          if (event.target.checked) {
                            setSelectedFilterValues((current) => [...current, option.value]);
                          } else {
                            setSelectedFilterValues((current) =>
                              current.filter((value) => value !== option.value),
                            );
                          }
                        }}
                      />
                    </label>
                  );
                })}
              </div>
            </div>
          ) : null}
        </section>
      ) : (
        <section className="rounded-lg border border-slate-800 bg-slate-900/70 p-4 text-sm text-slate-400">
          Loading schema metadata...
        </section>
      )}

      <section className="rounded-lg border border-slate-800 bg-slate-900/70 p-4">
        <h2 className="text-lg font-semibold">Results</h2>
        {crosstabQuery.loading ? <p className="mt-3 text-sm text-slate-400">Running crosstab query...</p> : null}
        {crosstabQuery.error ? (
          <p className="mt-3 rounded border border-red-500/40 bg-red-900/20 p-3 text-sm text-red-200">
            {crosstabQuery.error}
          </p>
        ) : null}

        {result && !crosstabQuery.loading ? (
          <div className="mt-3 overflow-x-auto">
            <table className="w-full text-left text-sm">
              <thead className="text-slate-400">
                <tr>
                  <th className="pb-2">{result.x}</th>
                  <th className="pb-2">{result.y}</th>
                  <th className="pb-2">Count</th>
                </tr>
              </thead>
              <tbody>
                {result.rows.map((row, index) => (
                  <tr key={`${String(row.x)}-${String(row.y)}-${index}`} className="border-t border-slate-800/80">
                    <td className="py-2 text-slate-200">{String(row.x ?? "NULL")}</td>
                    <td className="py-2 text-slate-200">{String(row.y ?? "NULL")}</td>
                    <td className="py-2 text-slate-300">{row.count.toLocaleString("en-US")}</td>
                  </tr>
                ))}
              </tbody>
            </table>
          </div>
        ) : null}
      </section>
    </div>
  );
}
</file>

<file path="src/routes/index.tsx">
import { createFileRoute } from "@tanstack/react-router";
import { useEffect, useMemo, useState } from "react";

import type { SchemaData } from "@/lib/api/contracts";
import { getSchema } from "@/lib/client/api";
import { useDuckDBQuery } from "@/lib/duckdb/use-query";
import { quoteIdentifier } from "@/lib/duckdb/sql-helpers";

export const Route = createFileRoute("/")({
  component: DashboardPage,
});

function percent(value: number): string {
  return `${(value * 100).toFixed(1)}%`;
}

function formatNumber(value: number): string {
  return new Intl.NumberFormat("en-US").format(value);
}

function asNumber(value: unknown, fallback = 0): number {
  if (typeof value === "number") return Number.isFinite(value) ? value : fallback;
  if (typeof value === "string") {
    const n = Number(value);
    return Number.isFinite(n) ? n : fallback;
  }
  return fallback;
}

function asNullableNumber(value: unknown): number | null {
  if (value == null) return null;
  const n = asNumber(value, Number.NaN);
  return Number.isNaN(n) ? null : n;
}

function DashboardPage() {
  const [schema, setSchema] = useState<SchemaData | null>(null);
  const [schemaError, setSchemaError] = useState<string | null>(null);
  const [selectedColumn, setSelectedColumn] = useState<string>("");

  useEffect(() => {
    let cancelled = false;

    void getSchema()
      .then((response) => {
        if (cancelled) return;
        setSchema(response.data);
        const preferred = response.data.columns.find((column) => column.name === "straightness");
        setSelectedColumn(preferred?.name ?? response.data.columns[0]?.name ?? "");
      })
      .catch((error: Error) => {
        if (!cancelled) setSchemaError(error.message);
      });

    return () => { cancelled = true; };
  }, []);

  const selectedMeta = useMemo(() => {
    if (!schema || !selectedColumn) return null;
    return schema.columns.find((c) => c.name === selectedColumn) ?? null;
  }, [schema, selectedColumn]);

  const isNumeric = selectedMeta?.logicalType === "numeric";
  const quoted = selectedColumn ? quoteIdentifier(selectedColumn) : "";

  const countsSql = useMemo(() => {
    if (!selectedColumn) return null;
    return `SELECT count(*)::BIGINT AS total_count, count(${quoted})::BIGINT AS non_null_count, (count(*) - count(${quoted}))::BIGINT AS null_count FROM data`;
  }, [selectedColumn, quoted]);

  const numericSql = useMemo(() => {
    if (!selectedColumn || !isNumeric) return null;
    return `SELECT avg(${quoted})::DOUBLE AS mean, stddev_samp(${quoted})::DOUBLE AS stddev, min(${quoted})::DOUBLE AS min_val, quantile_cont(${quoted}, 0.25)::DOUBLE AS p25, median(${quoted})::DOUBLE AS median_val, quantile_cont(${quoted}, 0.75)::DOUBLE AS p75, max(${quoted})::DOUBLE AS max_val FROM data WHERE ${quoted} IS NOT NULL`;
  }, [selectedColumn, isNumeric, quoted]);

  const categoricalSql = useMemo(() => {
    if (!selectedColumn || isNumeric) return null;
    return `SELECT cast(${quoted} AS VARCHAR) AS value, count(*)::BIGINT AS cnt FROM data WHERE ${quoted} IS NOT NULL GROUP BY 1 ORDER BY cnt DESC LIMIT 12`;
  }, [selectedColumn, isNumeric, quoted]);

  const countsQuery = useDuckDBQuery(countsSql);
  const numericQuery = useDuckDBQuery(numericSql);
  const categoricalQuery = useDuckDBQuery(categoricalSql);

  const statsLoading = countsQuery.loading || (isNumeric ? numericQuery.loading : categoricalQuery.loading);
  const statsError = countsQuery.error ?? (isNumeric ? numericQuery.error : categoricalQuery.error);

  const stats = useMemo(() => {
    if (!countsQuery.data) return null;
    const row = countsQuery.data.rows[0];
    if (!row) return null;

    const totalCount = asNumber(row[0]);
    const nonNullCount = asNumber(row[1]);
    const nullCount = asNumber(row[2]);
    const logicalType = selectedMeta?.logicalType ?? "unknown";

    if (isNumeric && numericQuery.data?.rows[0]) {
      const nr = numericQuery.data.rows[0];
      return {
        logicalType,
        stats: {
          kind: "numeric" as const,
          totalCount,
          nonNullCount,
          nullCount,
          mean: asNullableNumber(nr[0]),
          median: asNullableNumber(nr[4]),
          p25: asNullableNumber(nr[3]),
          p75: asNullableNumber(nr[5]),
        },
      };
    }

    if (!isNumeric && categoricalQuery.data) {
      const topValues = categoricalQuery.data.rows.map((r) => {
        const count = asNumber(r[1]);
        return {
          value: (r[0] ?? null) as string | number | boolean | null,
          count,
          percentage: nonNullCount > 0 ? Math.round((count / nonNullCount) * 10_000) / 100 : 0,
        };
      });

      return {
        logicalType,
        stats: {
          kind: "categorical" as const,
          totalCount,
          nonNullCount,
          nullCount,
          topValues,
        },
      };
    }

    return null;
  }, [countsQuery.data, numericQuery.data, categoricalQuery.data, isNumeric, selectedMeta]);

  const topMissingColumns = useMemo(() => {
    if (!schema) return [];
    return [...schema.columns].sort((a, b) => b.nullRatio - a.nullRatio).slice(0, 8);
  }, [schema]);

  return (
    <div className="space-y-8">
      <header className="space-y-2">
        <h1 className="text-3xl font-bold tracking-tight">Dataset Dashboard</h1>
        <p className="text-slate-300">
          Quick view of dataset shape, caveats, and single-column profile stats.
        </p>
      </header>

      {schemaError ? (
        <section className="rounded-lg border border-red-500/40 bg-red-900/20 p-4 text-red-200">
          Failed to load schema: {schemaError}
        </section>
      ) : null}

      {schema ? (
        <>
          <section className="grid gap-4 md:grid-cols-3">
            <div className="rounded-lg border border-slate-800 bg-slate-900/70 p-4">
              <p className="text-sm text-slate-400">Rows</p>
              <p className="mt-2 text-2xl font-semibold">{formatNumber(schema.dataset.rowCount)}</p>
            </div>
            <div className="rounded-lg border border-slate-800 bg-slate-900/70 p-4">
              <p className="text-sm text-slate-400">Columns</p>
              <p className="mt-2 text-2xl font-semibold">{formatNumber(schema.dataset.columnCount)}</p>
            </div>
            <div className="rounded-lg border border-slate-800 bg-slate-900/70 p-4">
              <p className="text-sm text-slate-400">Generated</p>
              <p className="mt-2 text-sm font-medium">
                {new Date(schema.dataset.generatedAt).toLocaleString()}
              </p>
            </div>
          </section>

          <section className="grid gap-6 lg:grid-cols-2">
            <div className="rounded-lg border border-slate-800 bg-slate-900/70 p-4">
              <h2 className="text-lg font-semibold">Global Caveats</h2>
              <div className="mt-4 space-y-3 text-sm">
                {schema.caveats.global.map((key) => {
                  const definition = schema.caveats.definitions.find((item) => item.key === key);
                  if (!definition) return null;

                  return (
                    <div key={key} className="rounded-md border border-slate-700 p-3">
                      <p className="font-medium text-slate-100">{definition.title}</p>
                      <p className="mt-1 text-slate-300">{definition.description}</p>
                      <p className="mt-1 text-xs text-slate-400">{definition.guidance}</p>
                    </div>
                  );
                })}
              </div>
            </div>

            <div className="rounded-lg border border-slate-800 bg-slate-900/70 p-4">
              <h2 className="text-lg font-semibold">Highest Missingness Columns</h2>
              <div className="mt-4 overflow-x-auto">
                <table className="w-full text-left text-sm">
                  <thead className="text-slate-400">
                    <tr>
                      <th className="pb-2">Column</th>
                      <th className="pb-2">Null Ratio</th>
                    </tr>
                  </thead>
                  <tbody>
                    {topMissingColumns.map((column) => (
                      <tr key={column.name} className="border-t border-slate-800/80">
                        <td className="py-2 pr-3 align-top text-slate-200">{column.name}</td>
                        <td className="py-2 text-slate-300">{percent(column.nullRatio)}</td>
                      </tr>
                    ))}
                  </tbody>
                </table>
              </div>
            </div>
          </section>

          <section className="rounded-lg border border-slate-800 bg-slate-900/70 p-4">
            <div className="flex flex-col gap-3 md:flex-row md:items-center md:justify-between">
              <h2 className="text-lg font-semibold">Column Stats</h2>
              <label className="flex items-center gap-2 text-sm text-slate-300">
                <span>Column</span>
                <select
                  className="rounded border border-slate-700 bg-slate-950 px-2 py-1"
                  value={selectedColumn}
                  onChange={(event) => setSelectedColumn(event.target.value)}
                >
                  {schema.columns.map((column) => (
                    <option key={column.name} value={column.name}>
                      {column.name}
                    </option>
                  ))}
                </select>
              </label>
            </div>

            {statsLoading ? <p className="mt-4 text-sm text-slate-400">Loading stats...</p> : null}
            {statsError ? (
              <p className="mt-4 rounded border border-red-500/40 bg-red-900/20 p-3 text-sm text-red-200">
                {statsError}
              </p>
            ) : null}

            {stats && !statsLoading ? (
              <div className="mt-4 space-y-4 text-sm">
                <div className="grid gap-3 md:grid-cols-4">
                  <StatCard label="Logical Type" value={stats.logicalType} />
                  <StatCard label="Total" value={formatNumber(stats.stats.totalCount)} />
                  <StatCard label="Non-null" value={formatNumber(stats.stats.nonNullCount)} />
                  <StatCard label="Null" value={formatNumber(stats.stats.nullCount)} />
                </div>

                {stats.stats.kind === "numeric" ? (
                  <div className="grid gap-3 md:grid-cols-4">
                    <StatCard label="Mean" value={stats.stats.mean?.toFixed(3) ?? "n/a"} />
                    <StatCard label="Median" value={stats.stats.median?.toFixed(3) ?? "n/a"} />
                    <StatCard label="P25" value={stats.stats.p25?.toFixed(3) ?? "n/a"} />
                    <StatCard label="P75" value={stats.stats.p75?.toFixed(3) ?? "n/a"} />
                  </div>
                ) : (
                  <div className="overflow-x-auto">
                    <table className="w-full text-left text-sm">
                      <thead className="text-slate-400">
                        <tr>
                          <th className="pb-2">Value</th>
                          <th className="pb-2">Count</th>
                          <th className="pb-2">Percent</th>
                        </tr>
                      </thead>
                      <tbody>
                        {stats.stats.topValues.map((row) => (
                          <tr key={`${row.value ?? "null"}`} className="border-t border-slate-800/80">
                            <td className="py-2 text-slate-200">{String(row.value ?? "NULL")}</td>
                            <td className="py-2 text-slate-300">{formatNumber(row.count)}</td>
                            <td className="py-2 text-slate-300">{row.percentage.toFixed(2)}%</td>
                          </tr>
                        ))}
                      </tbody>
                    </table>
                  </div>
                )}
              </div>
            ) : null}
          </section>
        </>
      ) : (
        <section className="rounded-lg border border-slate-800 bg-slate-900/70 p-4 text-sm text-slate-400">
          Loading schema metadata...
        </section>
      )}
    </div>
  );
}

function StatCard({ label, value }: { label: string; value: string }) {
  return (
    <div className="rounded-md border border-slate-800 bg-slate-950 p-3">
      <p className="text-xs uppercase tracking-wide text-slate-400">{label}</p>
      <p className="mt-1 text-base font-medium text-slate-100">{value}</p>
    </div>
  );
}
</file>

<file path="src/routes/profile.tsx">
import { createFileRoute } from "@tanstack/react-router";
import { useCallback, useEffect, useMemo, useState } from "react";

import type { SchemaData } from "@/lib/api/contracts";
import { getSchema } from "@/lib/client/api";
import { useDuckDB } from "@/lib/duckdb/provider";
import { quoteIdentifier, quoteLiteral } from "@/lib/duckdb/sql-helpers";

export const Route = createFileRoute("/profile")({
  component: ProfilePage,
});

interface ProfileSummary {
  totalSize: number;
  cohortSize: number;
  cohortSharePercent: number;
  uniquenessPercentile: number;
  percentileCards: Array<{
    metric: string;
    cohortMedian: number | null;
    globalPercentile: number | null;
  }>;
}

function asNumber(value: unknown, fallback = 0): number {
  if (typeof value === "number") return Number.isFinite(value) ? value : fallback;
  if (typeof value === "string") {
    const n = Number(value);
    return Number.isFinite(n) ? n : fallback;
  }
  return fallback;
}

function numberOrNull(value: unknown): number | null {
  if (value == null) return null;
  const n = asNumber(value, Number.NaN);
  return Number.isNaN(n) ? null : n;
}

function ProfilePage() {
  const { db } = useDuckDB();
  const [schema, setSchema] = useState<SchemaData | null>(null);
  const [schemaError, setSchemaError] = useState<string | null>(null);

  const [selectedColumns, setSelectedColumns] = useState<string[]>([]);
  const [selectedValues, setSelectedValues] = useState<Record<string, string>>({});
  const [valueOptionsByColumn, setValueOptionsByColumn] = useState<Record<string, string[]>>({});

  const [summary, setSummary] = useState<ProfileSummary | null>(null);
  const [runError, setRunError] = useState<string | null>(null);
  const [running, setRunning] = useState(false);

  useEffect(() => {
    let cancelled = false;

    void getSchema()
      .then((response) => {
        if (cancelled) return;
        setSchema(response.data);

        const demographicColumns = response.data.columns
          .filter((c) => c.tags.includes("demographic") && c.logicalType === "categorical")
          .slice(0, 3)
          .map((c) => c.name);

        setSelectedColumns(demographicColumns);
      })
      .catch((error: Error) => {
        if (!cancelled) setSchemaError(error.message);
      });

    return () => { cancelled = true; };
  }, []);

  // Fetch value options for each selected column using DuckDB-WASM
  useEffect(() => {
    if (selectedColumns.length === 0 || !db) return;

    let cancelled = false;

    void Promise.all(
      selectedColumns.map(async (column) => {
        const quoted = quoteIdentifier(column);
        const sql = `SELECT cast(${quoted} AS VARCHAR) AS value, count(*)::BIGINT AS cnt FROM data WHERE ${quoted} IS NOT NULL GROUP BY 1 ORDER BY cnt DESC LIMIT 12`;

        const conn = await db.connect();
        try {
          const result = await conn.query(sql);
          const values: string[] = [];
          for (let i = 0; i < result.numRows; i++) {
            values.push(String(result.getChildAt(0)?.get(i) ?? "NULL"));
          }
          return [column, values] as const;
        } finally {
          await conn.close();
        }
      }),
    )
      .then((entries) => {
        if (cancelled) return;
        const nextOptions: Record<string, string[]> = {};
        for (const [column, options] of entries) {
          nextOptions[column] = options;
        }
        setValueOptionsByColumn(nextOptions);
        setSelectedValues((current) => {
          const next: Record<string, string> = {};
          for (const column of selectedColumns) {
            const existing = current[column];
            if (existing && nextOptions[column]?.includes(existing)) {
              next[column] = existing;
            }
          }
          return next;
        });
      })
      .catch(() => {
        if (!cancelled) setValueOptionsByColumn({});
      });

    return () => { cancelled = true; };
  }, [selectedColumns, db]);

  const availableDemographicColumns = useMemo(() => {
    if (!schema) return [];
    return schema.columns.filter(
      (c) => c.tags.includes("demographic") && c.logicalType === "categorical",
    );
  }, [schema]);

  const filterPairs = useMemo(() => {
    return selectedColumns
      .map((column) => ({ column, value: selectedValues[column] }))
      .filter((item) => Boolean(item.column && item.value));
  }, [selectedColumns, selectedValues]);

  const canRun = filterPairs.length > 0 && !running && !!db;

  const runProfile = useCallback(async () => {
    if (filterPairs.length === 0 || !db) {
      setRunError("Select at least one demographic value before running profile analysis.");
      return;
    }

    setRunning(true);
    setRunError(null);

    const whereClause = `WHERE ${filterPairs
      .map((pair) => `${quoteIdentifier(pair.column)} = ${quoteLiteral(pair.value)}`)
      .join(" AND ")}`;

    const conn = await db.connect();

    try {
      const runSql = async (sql: string) => {
        const result = await conn.query(sql);
        const columns: string[] = result.schema.fields.map((f) => f.name);
        const rows: unknown[][] = [];
        for (let i = 0; i < result.numRows; i++) {
          const row: unknown[] = [];
          for (let c = 0; c < columns.length; c++) {
            let val = result.getChildAt(c)?.get(i);
            if (typeof val === "bigint") val = Number(val);
            row.push(val ?? null);
          }
          rows.push(row);
        }
        return { columns, rows };
      };

      const totalResult = await runSql("SELECT count(*)::BIGINT AS total_size FROM data");
      const cohortResult = await runSql(`SELECT count(*)::BIGINT AS cohort_size FROM data ${whereClause}`);

      const totalSize = numberOrNull(totalResult.rows[0]?.[0]) ?? 0;
      const cohortSize = numberOrNull(cohortResult.rows[0]?.[0]) ?? 0;
      const cohortSharePercent = totalSize > 0 ? (cohortSize / totalSize) * 100 : 0;
      const uniquenessPercentile = Math.max(0, Math.min(100, 100 - cohortSharePercent));

      const metricCandidates = [
        "totalfetishcategory",
        "powerlessnessvariable",
        "opennessvariable",
        "extroversionvariable",
        "neuroticismvariable",
      ].filter((metric) => schema?.columns.some((c) => c.name === metric));

      const percentileCards = await Promise.all(
        metricCandidates.map(async (metric) => {
          const metricSql = `
            WITH cohort AS (
              SELECT quantile_cont(${quoteIdentifier(metric)}, 0.5)::DOUBLE AS cohort_median
              FROM data
              ${whereClause} AND ${quoteIdentifier(metric)} IS NOT NULL
            )
            SELECT
              (SELECT cohort_median FROM cohort) AS cohort_median,
              CASE
                WHEN (SELECT cohort_median FROM cohort) IS NULL THEN NULL
                ELSE (
                  SELECT 100.0 *
                    SUM(CASE WHEN ${quoteIdentifier(metric)} <= (SELECT cohort_median FROM cohort) THEN 1 ELSE 0 END)::DOUBLE /
                    COUNT(*)::DOUBLE
                  FROM data
                  WHERE ${quoteIdentifier(metric)} IS NOT NULL
                )
              END AS percentile
          `;

          const metricResult = await runSql(metricSql);
          return {
            metric,
            cohortMedian: numberOrNull(metricResult.rows[0]?.[0]),
            globalPercentile: numberOrNull(metricResult.rows[0]?.[1]),
          };
        }),
      );

      setSummary({
        totalSize,
        cohortSize,
        cohortSharePercent,
        uniquenessPercentile,
        percentileCards,
      });
    } catch (error) {
      setRunError(error instanceof Error ? error.message : "Failed to run profile analysis.");
    } finally {
      await conn.close();
      setRunning(false);
    }
  }, [db, filterPairs, schema]);

  return (
    <div className="space-y-6">
      <header className="space-y-2">
        <h1 className="text-3xl font-bold tracking-tight">Profile Builder</h1>
        <p className="text-slate-300">
          Choose demographic answers and compute a people-like-you cohort with percentile summaries.
        </p>
      </header>

      {schemaError ? (
        <section className="rounded-lg border border-red-500/40 bg-red-900/20 p-4 text-red-200">
          Failed to load schema: {schemaError}
        </section>
      ) : null}

      {schema ? (
        <section className="rounded-lg border border-slate-800 bg-slate-900/70 p-4">
          <h2 className="text-lg font-semibold">Profile Inputs</h2>
          <div className="mt-4 grid gap-4 md:grid-cols-3">
            {[0, 1, 2].map((slot) => {
              const column = selectedColumns[slot] ?? "";
              const options = valueOptionsByColumn[column] ?? [];

              return (
                <div key={`slot-${slot}`} className="space-y-2 rounded border border-slate-800 bg-slate-950 p-3">
                  <label className="block text-sm text-slate-300">
                    Field {slot + 1}
                    <select
                      className="mt-1 w-full rounded border border-slate-700 bg-slate-900 px-2 py-2"
                      value={column}
                      onChange={(event) => {
                        const next = [...selectedColumns];
                        next[slot] = event.target.value;
                        setSelectedColumns(next.filter(Boolean));
                      }}
                    >
                      <option value="">Select a column</option>
                      {availableDemographicColumns.map((item) => (
                        <option key={item.name} value={item.name}>
                          {item.name}
                        </option>
                      ))}
                    </select>
                  </label>

                  <label className="block text-sm text-slate-300">
                    Value
                    <select
                      className="mt-1 w-full rounded border border-slate-700 bg-slate-900 px-2 py-2"
                      value={selectedValues[column] ?? ""}
                      onChange={(event) => {
                        const value = event.target.value;
                        setSelectedValues((current) => ({
                          ...current,
                          [column]: value,
                        }));
                      }}
                      disabled={!column || options.length === 0}
                    >
                      <option value="">Select a value</option>
                      {options.map((option) => (
                        <option key={option} value={option}>
                          {option}
                        </option>
                      ))}
                    </select>
                  </label>
                </div>
              );
            })}
          </div>

          <button
            type="button"
            onClick={() => {
              void runProfile();
            }}
            disabled={!canRun}
            className="mt-4 rounded border border-slate-700 bg-slate-100 px-4 py-2 text-sm font-medium text-slate-900 disabled:cursor-not-allowed disabled:opacity-50"
          >
            {running ? "Running..." : "Build profile"}
          </button>

          {runError ? (
            <p className="mt-4 rounded border border-red-500/40 bg-red-900/20 p-3 text-sm text-red-200">
              {runError}
            </p>
          ) : null}
        </section>
      ) : (
        <section className="rounded-lg border border-slate-800 bg-slate-900/70 p-4 text-sm text-slate-400">
          Loading schema metadata...
        </section>
      )}

      {summary ? (
        <section className="space-y-4 rounded-lg border border-slate-800 bg-slate-900/70 p-4">
          <h2 className="text-lg font-semibold">People-Like-You Summary</h2>

          <div className="grid gap-3 md:grid-cols-4">
            <SummaryCard label="Dataset Size" value={summary.totalSize.toLocaleString("en-US")} />
            <SummaryCard label="Cohort Size" value={summary.cohortSize.toLocaleString("en-US")} />
            <SummaryCard label="Cohort Share" value={`${summary.cohortSharePercent.toFixed(2)}%`} />
            <SummaryCard
              label="Uniqueness Percentile"
              value={`${summary.uniquenessPercentile.toFixed(2)}%`}
            />
          </div>

          <div className="grid gap-3 md:grid-cols-2 lg:grid-cols-3">
            {summary.percentileCards.map((card) => (
              <div key={card.metric} className="rounded border border-slate-800 bg-slate-950 p-3 text-sm">
                <p className="font-medium text-slate-100">{card.metric}</p>
                <p className="mt-2 text-slate-300">
                  Cohort median: {card.cohortMedian == null ? "n/a" : card.cohortMedian.toFixed(3)}
                </p>
                <p className="text-slate-400">
                  Global percentile: {card.globalPercentile == null ? "n/a" : `${card.globalPercentile.toFixed(2)}%`}
                </p>
              </div>
            ))}
          </div>
        </section>
      ) : null}
    </div>
  );
}

function SummaryCard({ label, value }: { label: string; value: string }) {
  return (
    <div className="rounded border border-slate-800 bg-slate-950 p-3">
      <p className="text-xs uppercase tracking-wide text-slate-400">{label}</p>
      <p className="mt-1 text-base font-medium text-slate-100">{value}</p>
    </div>
  );
}
</file>

<file path="src/routes/sql.tsx">
import { createFileRoute } from "@tanstack/react-router";
import { useCallback, useEffect, useMemo, useState } from "react";

import type { SchemaData } from "@/lib/api/contracts";
import { getSchema } from "@/lib/client/api";
import { useDuckDB } from "@/lib/duckdb/provider";

export const Route = createFileRoute("/sql")({
  component: SqlConsolePage,
});

interface QueryResult {
  columns: string[];
  rows: unknown[][];
}

const starterSql = `SELECT
  straightness,
  politics,
  COUNT(*)::BIGINT AS respondents
FROM data
GROUP BY 1, 2
ORDER BY respondents DESC`;

function SqlConsolePage() {
  const { db } = useDuckDB();
  const [schema, setSchema] = useState<SchemaData | null>(null);
  const [search, setSearch] = useState("");

  const [sql, setSql] = useState(starterSql);
  const [limit, setLimit] = useState(1000);
  const [running, setRunning] = useState(false);
  const [error, setError] = useState<string | null>(null);
  const [result, setResult] = useState<QueryResult | null>(null);
  const [rowCount, setRowCount] = useState<number | undefined>();

  useEffect(() => {
    let cancelled = false;

    void getSchema()
      .then((response) => {
        if (!cancelled) setSchema(response.data);
      })
      .catch(() => {
        if (!cancelled) setSchema(null);
      });

    return () => { cancelled = true; };
  }, []);

  const filteredColumns = useMemo(() => {
    if (!schema) return [];
    if (!search.trim()) return schema.columns.slice(0, 200);
    const term = search.toLowerCase();
    return schema.columns.filter((c) => c.name.toLowerCase().includes(term)).slice(0, 200);
  }, [schema, search]);

  const execute = useCallback(async () => {
    if (!db) return;

    setRunning(true);
    setError(null);

    const conn = await db.connect();

    try {
      // Wrap user query with limit
      const limitedSql = `SELECT * FROM (${sql.trim().replace(/;+$/g, "")}) AS bounded_query LIMIT ${limit}`;
      const arrowResult = await conn.query(limitedSql);

      const columns = arrowResult.schema.fields.map((f) => f.name);
      const rows: unknown[][] = [];

      for (let i = 0; i < arrowResult.numRows; i++) {
        const row: unknown[] = [];
        for (let c = 0; c < columns.length; c++) {
          let val = arrowResult.getChildAt(c)?.get(i);
          if (typeof val === "bigint") val = Number(val);
          row.push(val ?? null);
        }
        rows.push(row);
      }

      setResult({ columns, rows });
      setRowCount(rows.length);
    } catch (executionError) {
      setError(executionError instanceof Error ? executionError.message : "Query failed");
      setResult(null);
      setRowCount(undefined);
    } finally {
      await conn.close();
      setRunning(false);
    }
  }, [db, sql, limit]);

  function exportCsv() {
    if (!result) return;

    const escaped = (value: unknown) => {
      const text = String(value ?? "");
      return `"${text.replaceAll('"', '""')}"`;
    };

    const header = result.columns.map(escaped).join(",");
    const lines = result.rows.map((row) => row.map(escaped).join(","));
    const csv = [header, ...lines].join("\n");

    const blob = new Blob([csv], { type: "text/csv;charset=utf-8" });
    const href = URL.createObjectURL(blob);
    const link = document.createElement("a");
    link.href = href;
    link.download = `bks-query-${Date.now()}.csv`;
    link.click();
    URL.revokeObjectURL(href);
  }

  return (
    <div className="space-y-6">
      <header className="space-y-2">
        <h1 className="text-3xl font-bold tracking-tight">SQL Console</h1>
        <p className="text-slate-300">Run read-only DuckDB SQL with row limits and CSV export.</p>
      </header>

      <div className="grid gap-4 lg:grid-cols-[320px,1fr]">
        <aside className="rounded-lg border border-slate-800 bg-slate-900/70 p-4">
          <h2 className="text-base font-semibold">Schema</h2>
          <input
            className="mt-3 w-full rounded border border-slate-700 bg-slate-950 px-2 py-2 text-sm"
            placeholder="Search columns"
            value={search}
            onChange={(event) => setSearch(event.target.value)}
          />
          <div className="mt-3 max-h-[420px] overflow-auto pr-1 text-sm">
            {filteredColumns.map((column) => (
              <button
                key={column.name}
                type="button"
                className="block w-full truncate rounded px-2 py-1 text-left text-slate-300 hover:bg-slate-800 hover:text-white"
                onClick={() => {
                  setSql((current) => `${current}\n-- ${column.name}`);
                }}
                title={column.name}
              >
                {column.name}
              </button>
            ))}
          </div>
        </aside>

        <section className="rounded-lg border border-slate-800 bg-slate-900/70 p-4">
          <label className="block text-sm text-slate-300" htmlFor="sql-editor">
            SQL
          </label>
          <textarea
            id="sql-editor"
            className="mt-2 h-56 w-full rounded border border-slate-700 bg-slate-950 p-3 font-mono text-sm"
            value={sql}
            onChange={(event) => setSql(event.target.value)}
          />

          <div className="mt-3 flex flex-wrap items-center gap-3">
            <label className="text-sm text-slate-300">
              Limit
              <input
                type="number"
                min={1}
                max={10000}
                className="ml-2 w-24 rounded border border-slate-700 bg-slate-950 px-2 py-1"
                value={limit}
                onChange={(event) => setLimit(Math.max(1, Math.min(10000, Number(event.target.value) || 1)))}
              />
            </label>

            <button
              type="button"
              onClick={() => {
                void execute();
              }}
              disabled={running || !db}
              className="rounded border border-slate-700 bg-slate-100 px-4 py-2 text-sm font-medium text-slate-900 disabled:cursor-not-allowed disabled:opacity-50"
            >
              {running ? "Running..." : "Run query"}
            </button>

            <button
              type="button"
              onClick={exportCsv}
              disabled={!result}
              className="rounded border border-slate-700 bg-slate-800 px-4 py-2 text-sm font-medium text-slate-100 disabled:cursor-not-allowed disabled:opacity-50"
            >
              Export CSV
            </button>

            {rowCount != null ? (
              <span className="text-xs text-slate-400">
                Rows: {rowCount} | Limit: {limit}
              </span>
            ) : null}
          </div>

          {error ? (
            <p className="mt-4 rounded border border-red-500/40 bg-red-900/20 p-3 text-sm text-red-200">
              {error}
            </p>
          ) : null}

          {result ? (
            <div className="mt-4 max-h-[420px] overflow-auto rounded border border-slate-800">
              <table className="w-full border-collapse text-left text-sm">
                <thead className="bg-slate-950 text-slate-300">
                  <tr>
                    {result.columns.map((column) => (
                      <th key={column} className="border-b border-slate-800 px-3 py-2 font-medium">
                        {column}
                      </th>
                    ))}
                  </tr>
                </thead>
                <tbody>
                  {result.rows.map((row, rowIndex) => (
                    <tr key={`row-${rowIndex}`} className="border-b border-slate-800/70 text-slate-200">
                      {row.map((value, columnIndex) => (
                        <td key={`cell-${rowIndex}-${columnIndex}`} className="px-3 py-2 align-top">
                          {String(value ?? "NULL")}
                        </td>
                      ))}
                    </tr>
                  ))}
                </tbody>
              </table>
            </div>
          ) : null}
        </section>
      </div>
    </div>
  );
}
</file>

<file path="src/router.tsx">
import { createRouter } from "@tanstack/react-router";
import { routeTree } from "./routeTree.gen";

export function getRouter() {
  const router = createRouter({
    routeTree,
    scrollRestoration: true,
  });

  return router;
}

declare module "@tanstack/react-router" {
  interface Register {
    router: ReturnType<typeof getRouter>;
  }
}
</file>

<file path="src/styles.css">
@import "tailwindcss";

body {
  @apply m-0;
  font-family: -apple-system, BlinkMacSystemFont, "Segoe UI", "Roboto", "Oxygen",
    "Ubuntu", "Cantarell", "Fira Sans", "Droid Sans", "Helvetica Neue",
    sans-serif;
  -webkit-font-smoothing: antialiased;
  -moz-osx-font-smoothing: grayscale;
}

code {
  font-family: source-code-pro, Menlo, Monaco, Consolas, "Courier New",
    monospace;
}
</file>

<file path=".gitignore">
node_modules
.DS_Store
dist
dist-ssr
*.local
count.txt
.env
.nitro
.tanstack
.wrangler
.output
.vinxi
__unconfig*
todos.json
</file>

<file path=".mcp.json">
{
  "mcpServers": {
    "tanstack": {
      "command": "npx",
      "args": ["@tanstack/cli", "mcp"]
    },
    "Railway": {
      "command": "npx",
      "args": ["-y", "@railway/mcp-server"]
    }
  }
}
</file>

<file path=".npmrc">
package-manager-strict-version=false
manage-package-manager-versions=true
</file>

<file path="META-PLAN.md">
# Meta-Plan: Agent-First Development Philosophy

This document captures the higher-level engineering philosophy for this project, informed by [OpenAI's harness engineering approach](https://openai.com/index/harness-engineering/) and patterns from the solstice codebase. It sits alongside `PLAN.md` (the concrete implementation plan).

## Core Principle

**Humans steer. Agents execute.**

The primary job is not to write code, but to design environments, specify intent, and build feedback loops that let agents do reliable work. When something fails, the fix is never "try harder" — it's "what capability is missing, and how do we make it legible and enforceable?"

## Repository as System of Record

Anything not in the repo doesn't exist for agents. Slack discussions, mental models, undocumented decisions — if an agent can't discover it, it's illegible.

### Knowledge Architecture

```
kink/
├── CLAUDE.md              # ~100 lines, table of contents (NOT an encyclopedia)
├── PLAN.md                # Concrete implementation plan
├── META-PLAN.md           # This file: philosophy and patterns
├── docs/
│   ├── design/            # Architectural decisions, core beliefs
│   ├── schema/            # Column metadata, data dictionary
│   ├── plans/
│   │   ├── active/        # In-progress execution plans
│   │   └── completed/     # Done plans (for context)
│   └── references/        # External docs pulled in-repo (llms.txt files, etc)
```

**Progressive disclosure**: CLAUDE.md is the entry point — a map with pointers. Agents start small and are taught where to look next, not overwhelmed upfront.

## CLAUDE.md Philosophy

Following the lesson: "We tried the one big AGENTS.md approach. It failed."

CLAUDE.md should be:
- **Short** (~100 lines) — a table of contents, not a manual
- **Stable** — change rarely, point to things that change often
- **Navigational** — tell agents where to look, not what to do

It should contain:
1. Project overview (2-3 sentences)
2. Stack and key tools
3. How to run/build/test (exact commands)
4. Pointers to docs/ for deeper context
5. Architectural invariants (the few rules that matter everywhere)
6. Pre-commit requirements

It should NOT contain:
- Detailed implementation guides (put in docs/)
- Long lists of conventions (encode as linters)
- Task-specific instructions (put in execution plans)

## Mechanical Enforcement Over Documentation

> "When documentation falls short, promote the rule into code."

Prefer linting rules and structural tests over written guidelines. An enforced rule is a rule that works; a documented rule is a suggestion.

### Enforcement Stack (adapted from solstice)

1. **oxlint** — Fast, catches common issues (no-console, eqeqeq, no-debugger)
2. **ESLint** — Framework-specific rules (TanStack Router, React hooks)
3. **TypeScript strict mode** — Type safety as enforcement
4. **Pre-commit hooks** (husky + lint-staged):
   - Format staged files
   - Type check (`pnpm check-types`)
   - Run tests (`pnpm test --run`)
5. **CI pipeline** — Lint, type-check, test, build on every push

### Custom Lints for Agent Legibility

Write custom lint error messages that inject remediation instructions into agent context. When an agent hits a lint failure, the error message should tell it exactly how to fix it.

## Agent Safety

### Git Safety Guard (from solstice)

A `.claude/hooks/git_safety_guard.py` that blocks destructive operations:
- `git reset --hard`
- `git clean -f` (except dry-run)
- `force push`
- `rm -rf` (except /tmp)

This prevents agents from accidentally destroying work.

## Agent Workflows (from solstice .claude/commands/)

Encode common workflows as reusable commands:

| Command | Purpose |
|---|---|
| `/build` | Scaffold → implement → validate loop |
| `/investigate` | Systematic exploration with evidence gathering |
| `/review` | Code review with scope confirmation |
| `/refactor` | Find patterns, plan refactoring, preserve behavior |

These are markdown files in `.claude/commands/` that structure agent work into phases.

## Architecture Enforcement

> "Enforce boundaries centrally, allow autonomy locally."

### Invariants for This Project

1. **Data flows one direction**: Parquet → DuckDB → Query → Component
2. **API routes are thin**: They translate HTTP to DuckDB SQL and return JSON. No business logic in routes.
3. **Schema is the source of truth**: `src/lib/schema/columns.ts` defines all column metadata. Charts, filters, and API all derive from it.
4. **Client-first**: DuckDB-WASM handles all human-facing queries. Server API routes exist only for AI agents.
5. **Validate at boundaries**: API inputs validated with Zod. SQL queries sanitized. No YOLO data probing.

## Making the App Legible to Agents

Borrowing from OpenAI's approach of making the app itself inspectable by agents:

1. **API routes serve structured JSON** — agents can query the data directly
2. **Schema endpoint** — `/api/schema` returns column metadata, types, categories, valid values
3. **MCP server** — agents connect directly and query with typed tools
4. **Descriptive column metadata** — every column has a human+agent readable description
5. **Query examples in docs** — show agents what queries are useful

## Boring Technology, Agent-Friendly

> "Technologies often described as boring tend to be easier for agents to model."

Our stack choices optimize for agent legibility:
- **DuckDB SQL** — standard SQL, well-represented in training data
- **React** — most common UI framework, agents know it deeply
- **Tailwind** — utility classes are predictable and composable
- **shadcn/ui** — copy-paste components, no opaque abstractions
- **Zod** — schema validation agents naturally reach for
- **Parquet** — standard columnar format, DuckDB's native format

## Continuous Cleanup (Garbage Collection)

> "Technical debt is like a high-interest loan."

Rather than "AI slop Fridays," encode golden principles and run cleanup continuously:

1. **Quality scoring** — grade each feature area, track gaps
2. **Doc gardening** — periodic checks for stale documentation
3. **Pattern consistency** — prefer shared utilities over hand-rolled helpers
4. **Small PRs** — corrections are cheap when changes are small

## Testing Strategy

### Unit Tests (Vitest)
- jsdom environment for component tests
- Test utilities and schema logic
- Coverage reporting

### E2E / Visual Validation
- Playwright for critical user journeys
- Screenshot evidence in `.playwright-mcp/`
- Accessibility checks

### Agent-Driven Validation
- Agents can boot the app, drive it with Chrome DevTools, validate their own work
- API routes are self-documenting and testable via curl
- MCP server tools are individually testable

## Pre-commit Contract

Every commit must pass:
1. `lint-staged` (format + lint staged files)
2. `pnpm check-types` (TypeScript)
3. `pnpm test --run` (all unit tests)

This is non-negotiable. It's the minimum bar that keeps agents from committing broken code.

## What This Means in Practice

When building a feature:
1. Write the execution plan (docs/plans/active/)
2. Define the schema/types first
3. Implement with mechanical enforcement (lints, types, tests)
4. Validate via API + UI
5. Move plan to completed when done

When something breaks:
1. Don't "try harder" — ask what's missing
2. If the agent can't find context → add it to docs
3. If the agent makes a mistake → add a lint rule
4. If the pattern drifts → add a structural test

The goal is a codebase that gets easier for agents to work in over time, not harder.
</file>

<file path="package.json">
{
  "name": "kink-survey-explorer",
  "private": true,
  "type": "module",
  "scripts": {
    "dev": "vite dev --port 3000",
    "build": "vite build",
    "preview": "vite preview",
    "start": "node .output/server/index.mjs",
    "sync-public-data": "node scripts/sync-public-data.mjs",
    "profile-schema": "node scripts/profile-schema.mjs",
    "check-types": "tsc --noEmit",
    "test": "vitest",
    "lint": "oxlint",
    "prepare": "husky"
  },
  "dependencies": {
    "@duckdb/duckdb-wasm": "1.33.1-dev18.0",
    "@duckdb/node-api": "1.4.4-r.1",
    "@tailwindcss/vite": "^4.1.18",
    "@tanstack/react-devtools": "^0.7.0",
    "@tanstack/react-router": "^1.132.0",
    "@tanstack/react-router-devtools": "^1.132.0",
    "@tanstack/react-router-ssr-query": "^1.131.7",
    "@tanstack/react-start": "^1.132.0",
    "@tanstack/router-plugin": "^1.132.0",
    "lucide-react": "^0.545.0",
    "nitro": "npm:nitro-nightly@3.0.1-20260212-205859-0f4c665f",
    "react": "^19.2.0",
    "react-dom": "^19.2.0",
    "tailwindcss": "^4.1.18",
    "vite-tsconfig-paths": "^6.0.2",
    "zod": "^4.3.6"
  },
  "devDependencies": {
    "@tanstack/devtools-vite": "^0.3.11",
    "@testing-library/dom": "^10.4.0",
    "@testing-library/react": "^16.2.0",
    "@types/node": "^22.10.2",
    "@types/react": "^19.2.0",
    "@types/react-dom": "^19.2.0",
    "@vitejs/plugin-react": "^5.0.4",
    "husky": "^9.1.7",
    "jsdom": "^27.0.0",
    "lint-staged": "^16.2.7",
    "oxlint": "^1.47.0",
    "typescript": "^5.7.2",
    "vite": "^7.1.7",
    "vitest": "^3.0.5"
  },
  "lint-staged": {
    "*.{ts,tsx}": [
      "oxlint"
    ]
  },
  "packageManager": "pnpm@10.6.3",
  "pnpm": {
    "onlyBuiltDependencies": [
      "esbuild"
    ]
  }
}
</file>

<file path="PLAN.md">
# Big Kink Survey Explorer - Implementation Plan (v1)

Last updated: 2026-02-12

## 1) Product Goal

Build a production-ready web app for exploring the Big Kink Survey sample dataset and expose the same data model to AI agents through HTTP APIs and an MCP server.

Primary users:
- Human users exploring distributions, relationships, and profile comparisons.
- AI agents needing structured, queryable access to schema and summary statistics.

## 2) Ground Truth and Constraints

- Dataset: `data/BKSPublic.parquet` (15,511 rows x 376 columns).
- Cleaning caveats from source docs must be surfaced in UI/API docs:
  - Some variables are binned/combined/computed.
  - Many questions are gated; missing values are often structural, not random.
  - Some questions were added later, so missingness can be cohort/time related.
- No raw free-form SQL should run without safety limits on server or MCP.
- Python tooling must use `uv`.

## 3) Scope

### In Scope (v1)

- TanStack Start app with 4 pages:
  - `/` dashboard
  - `/explore` cross-tab explorer
  - `/profile` profile builder
  - `/sql` SQL console
- DuckDB-WASM for in-browser querying of parquet.
- Server API routes backed by DuckDB (Node).
- Python MCP server exposing safe analytics tools.
- Railway deployment for app service + MCP service.

### Out of Scope (v1)

- User accounts/auth.
- Editable/saved dashboards.
- Real-time collaboration.
- Multi-dataset upload.
- Causal inference features.

## 4) Success Criteria

### User Experience
- Initial app shell visible in < 2s on broadband desktop.
- DuckDB-WASM ready in < 5s on first load for typical laptop.
- Core charts update after filter change in < 1s for common queries.

### API / Agent Reliability
- `GET /api/schema` p95 < 500ms.
- `GET /api/stats/:column` p95 < 1s.
- `POST /api/query` p95 < 2s for bounded queries.
- API and MCP return typed errors (not raw stack traces).

### Correctness
- Shared column metadata used consistently by UI, API, and MCP.
- Structural-missingness caveats are visible in docs and API responses where relevant.

## 5) Architecture

### Web App
- TanStack Start + React + Tailwind v4 + shadcn/ui.
- DuckDB-WASM in browser for interactive client-side analytics.
- Feature modules under `src/features/*`; thin route files in `src/routes/*`.

### Server API
- TanStack Start/Nitro server handlers for `/api/*`.
- DuckDB Node bindings to query `data/BKSPublic.parquet`.
- Read-only query model with query guards and row/time limits.

### MCP Service
- Python MCP server (`mcp-server/server.py`) using DuckDB Python bindings.
- Tools return structured JSON matching API metadata conventions.

### Deployment Model
- Railway Service A: web app runtime (not static-only, because `/api/*` is dynamic).
- Railway Service B: MCP container runtime.

## 6) Canonical Project Layout

```text
kink/
├── data/
│   ├── BKSPublic.parquet
│   ├── BKSPublic_column_notes.txt
│   └── Big Kink Survey (970k cleaned).md
├── public/
│   └── BKSPublic.parquet                 # copied at build or predev script
├── src/
│   ├── routes/
│   │   ├── __root.tsx
│   │   ├── index.tsx
│   │   ├── explore.tsx
│   │   ├── profile.tsx
│   │   ├── sql.tsx
│   │   └── api/
│   │       ├── schema.ts
│   │       ├── query.ts
│   │       ├── stats.$column.ts
│   │       └── crosstab.ts
│   ├── features/
│   │   ├── dashboard/
│   │   ├── explorer/
│   │   ├── profile/
│   │   └── sql-console/
│   ├── lib/
│   │   ├── duckdb/
│   │   │   ├── init.ts
│   │   │   ├── provider.tsx
│   │   │   └── use-query.ts
│   │   ├── schema/
│   │   │   ├── columns.generated.json
│   │   │   ├── caveats.ts
│   │   │   └── categories.ts
│   │   └── api/
│   │       └── contracts.ts
│   └── components/ui/
├── scripts/
│   ├── profile-schema.ts                 # generate columns.generated.json
│   └── sync-public-data.ts               # copy parquet to public/
├── mcp-server/
│   ├── pyproject.toml
│   ├── server.py
│   └── Dockerfile
├── package.json
└── railway.toml
```

## 7) Milestones (Delivery-Oriented)

### M0 - Scaffold and Data Profiling

Deliverables:
- Project scaffold with TanStack Start, Tailwind v4, shadcn/ui baseline.
- `scripts/sync-public-data.ts` to copy parquet to `public/`.
- `scripts/profile-schema.ts` to generate `columns.generated.json` containing:
  - name
  - inferred logical type (`categorical|numeric|boolean|text|unknown`)
  - null ratio
  - approx cardinality
  - category tags (`demographic|ocean|fetish|derived|other`)

Acceptance criteria:
- `pnpm dev` runs.
- Parquet is loadable from browser and server contexts.
- `columns.generated.json` checked in and reproducible.

### M1 - Shared Data Contract Layer

Deliverables:
- `src/lib/api/contracts.ts` with Zod schemas for all API request/response shapes.
- `src/lib/schema/caveats.ts` with documented caveats (binned/combined/gated/late-added).
- Reusable utilities for formatting null/implicit-zero caveat hints.

Acceptance criteria:
- API handlers and UI both consume same contracts.
- Invalid API payloads return `400` with structured error object.

### M2 - Vertical Slice (UI + API)

Deliverables:
- Dashboard route with 3-5 stable charts using client-side DuckDB-WASM.
- `GET /api/schema` fully implemented from generated metadata.
- One end-to-end flow: pick a variable, fetch summary, render chart.

Acceptance criteria:
- Manual smoke test passes for dashboard and `/api/schema`.
- No chart crashes on high-null columns.

### M3 - Explorer, Profile, and SQL Console

Deliverables:
- `/explore`:
  - x/y variable pickers
  - auto chart mapping:
    - categorical x categorical -> heatmap/table
    - categorical x numeric -> box/violin summary
    - numeric x numeric -> scatter/hexbin fallback
  - demographic filters
- `/profile`:
  - input controls for selected demographic and preference columns
  - "people like you" percentile summaries
- `/sql`:
  - editor + schema sidebar + results grid + CSV export

Acceptance criteria:
- Query cancellation works for long-running client-side queries.
- All 3 pages handle empty/NA-heavy results gracefully.

### M4 - API Hardening

Deliverables:
- `POST /api/query`, `GET /api/stats/:column`, `GET /api/crosstab`.
- Query guardrails:
  - read-only allowlist (`SELECT`, `WITH`, `DESCRIBE`, `EXPLAIN` optional)
  - deny mutating statements
  - max rows (default 1,000; hard cap 10,000)
  - timeout (e.g., 5s)
- Standard response envelope:
  - `{ ok: true, data, meta }`
  - `{ ok: false, error: { code, message, details? } }`

Acceptance criteria:
- Negative tests confirm blocked unsafe SQL.
- API p95 latency targets met locally on representative queries.

### M5 - MCP Server

Deliverables:
- `mcp-server/server.py` tools:
  - `get_schema`
  - `get_stats`
  - `cross_tabulate`
  - `query_data` (bounded, read-only)
  - `search_columns`
- Tool schemas align with API contracts where possible.
- Dockerfile + Railway run command.

Acceptance criteria:
- MCP client can call each tool successfully.
- Errors are typed and actionable.

### M6 - Deploy and Operate

Deliverables:
- Railway config for web + MCP services.
- Health checks and startup probes.
- Basic observability:
  - request logs
  - error logs with correlation ids
- README runbook for local/dev/prod and rollback steps.

Acceptance criteria:
- Fresh deploy from main branch succeeds.
- Smoke tests pass against production URLs.

## 8) API Contract (v1)

### `GET /api/schema`
Returns list of columns with metadata and caveats.

### `POST /api/query`
Request:
- `sql: string`
- `limit?: number`

Behavior:
- Enforce read-only SQL.
- Apply default limit if absent.

### `GET /api/stats/:column`
Returns:
- numeric: count, nulls, mean, stddev, min, p25, median, p75, max
- categorical: count, nulls, top categories with percentages
- caveat flags when column is binned/combined/computed

### `GET /api/crosstab?x=...&y=...&filters=...`
Returns cross-tab matrix plus marginal totals and null handling metadata.

## 9) Testing Strategy

### Unit
- Query guard parser/validator.
- Metadata mapper.
- Chart-type resolver logic.

### Integration
- API route tests using sample queries.
- DuckDB initialization and parquet loading.

### E2E (Playwright)
- Dashboard loads and renders charts.
- Explorer variable swap updates visualization.
- SQL console executes safe query and exports CSV.

### Smoke (Post-deploy)
- `/`
- `/api/schema`
- `/api/stats/age`
- MCP `get_schema`

## 10) Risk Register and Mitigations

- DuckDB-WASM memory pressure on low-end devices.
  - Mitigation: lazy-load heavy views, fallback to server API mode toggle.
- Misinterpretation from structural missingness.
  - Mitigation: explicit caveat labels in tooltips/API metadata.
- SQL abuse or runaway queries.
  - Mitigation: strict read-only checks, timeout, row caps.
- Schema drift between data and metadata.
  - Mitigation: metadata generation script in CI/check step.
- Sensitive-content handling and moderation concerns.
  - Mitigation: clear content warning and neutral wording in UI labels.

## 11) Implementation Sequence (Recommended)

1. M0 scaffold + metadata generation.
2. M1 contracts and caveat model.
3. M2 vertical slice to de-risk integration.
4. M3 feature pages.
5. M4 API hardening.
6. M5 MCP service.
7. M6 deployment + runbook.

## 12) Definition of Done (v1 Release)

- All milestone acceptance criteria satisfied.
- Core routes and API endpoints documented.
- Deploy pipeline green for app + MCP.
- Manual exploratory QA complete with no P0/P1 bugs.

## 13) Open Decisions (Resolved)

- **Public API scope**: Expose row-level query results broadly. All API endpoints (`/api/query`, `/api/stats`, `/api/crosstab`, `/api/schema`) are publicly available with read-only guardrails and row limits.
- **SQL console availability**: Available in production to all users, no feature flag. Guardrails (read-only, row cap, timeout) provide sufficient safety.
- **MCP service exposure**: Internet-exposed with no auth restriction. Same read-only guardrails as the API apply.
</file>

<file path="tsconfig.json">
{
  "include": ["**/*.ts", "**/*.tsx"],
  "compilerOptions": {
    "target": "ES2022",
    "jsx": "react-jsx",
    "module": "ESNext",
    "lib": ["ES2022", "DOM", "DOM.Iterable"],
    "types": ["vite/client"],

    /* Bundler mode */
    "moduleResolution": "bundler",
    "allowImportingTsExtensions": true,
    "resolveJsonModule": true,
    "verbatimModuleSyntax": false,
    "noEmit": true,

    /* Linting */
    "skipLibCheck": true,
    "strict": true,
    "noUnusedLocals": true,
    "noUnusedParameters": true,
    "noFallthroughCasesInSwitch": true,
    "noUncheckedSideEffectImports": true,
    "baseUrl": ".",
    "paths": {
      "@/*": ["./src/*"]
    }
  }
}
</file>

<file path="vite.config.ts">
import { defineConfig } from 'vite'
import { tanstackStart } from '@tanstack/react-start/plugin/vite'
import { nitro } from 'nitro/vite'
import viteReact from '@vitejs/plugin-react'
import viteTsConfigPaths from 'vite-tsconfig-paths'
import tailwindcss from '@tailwindcss/vite'

export default defineConfig({
  plugins: [
    viteTsConfigPaths({
      projects: ['./tsconfig.json'],
    }),
    tailwindcss(),
    tanstackStart(),
    nitro(),
    viteReact(),
  ],
})
</file>

<file path="CLAUDE.md">
# Big Kink Survey Explorer

Interactive data explorer for the Big Kink Survey (15.5k rows, 365 columns). Serves both human users (browser UI) and AI agents (REST API + MCP server).

## Stack
- TanStack Start + React, Tailwind v4, Vite 7 + Nitro
- DuckDB CLI + parquet for web/API queries, DuckDB Python for MCP server
- `pnpm` for JS, `uv` for Python (never pip directly)

## Commands
- `pnpm dev` — start dev server (port 3000)
- `pnpm build` — production build (outputs to `.output/`)
- `pnpm start` — run production server (`node .output/server/index.mjs`)
- `pnpm sync-public-data` — copy parquet from `data/` to `public/`
- `pnpm profile-schema` — regenerate `src/lib/schema/columns.generated.json`
- `pnpm check-types` — TypeScript validation
- `pnpm test --run` — unit tests
- `pnpm lint` — oxlint + eslint

## Pre-commit Contract
Every commit must pass: lint-staged → type-check → tests

## Key Files
- `PLAN.md` — implementation plan with milestones M0-M6
- `META-PLAN.md` — agent-first development philosophy
- `data/` — source parquet + column notes + survey documentation
- `src/router.tsx` — TanStack Start entry (required, exports `getRouter`)
- `src/routes/api/` — server route API endpoints
- `mcp-server/` — Python MCP server for AI agent access

## Docs (progressive disclosure — start here, dig in as needed)
- `docs/design/frontend.md` — **design system**: "Ink & Paper" editorial aesthetic, color tokens, typography, component patterns
- `docs/design/architecture.md` — stack decisions, technical rationale
- `docs/design/deployment.md` — Railway config, URLs, how to deploy
- `docs/design/mcps.md` — available MCP servers and how to use them
- `docs/schema/README.md` — schema metadata and caveat generation model
- `docs/plans/active/` — in-progress execution plans
- `docs/plans/completed/` — finished plans for context

## Deployment
- **Live**: https://bks-explorer-production.up.railway.app
- Railway project: `bks-explorer`, environment: `production`
- Deploy: `railway up` or use Railway MCP `deploy` tool

## Architectural Invariants
1. Data flows one direction: Parquet → DuckDB → Query → Component
2. API routes are thin wrappers around DuckDB SQL
3. Schema metadata is the single source of truth for columns
4. UI and agents both consume typed API contracts
5. Validate at boundaries (Zod for API inputs)
6. UI follows "Ink & Paper" design system — see `docs/design/frontend.md` for tokens/patterns

## Session Hygiene
At the end of each session or before compaction, update `docs/` with what was built, changed, or decided. Future agents should be able to read CLAUDE.md → docs/ and understand the current state without needing conversation history.
</file>

</files>
